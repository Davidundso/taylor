{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking NAdamW behaviour\n",
    "- Checking if NAadamW can be mathematically reduced to alpha * d (a stepsize and a direction)\n",
    "\n",
    "### (1) Define NAdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "absolute_path = \"/home/suckrowd/Documents/taylor\"\n",
    "if absolute_path not in sys.path:\n",
    "    sys.path.append(absolute_path)\n",
    "    \n",
    "import math\n",
    "from typing import Dict, Iterator, List, Tuple\n",
    "\n",
    "from absl import logging\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.distributed.nn as dist_nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torch.optim.lr_scheduler import SequentialLR\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import time\n",
    "from algorithmic_efficiency import spec\n",
    "from algorithmic_efficiency.pytorch_utils import pytorch_setup\n",
    "from source.adaptive_optimizer import AdaptiveLROptimizer\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from curvlinops import GGNLinearOperator\n",
    "import csv\n",
    "\n",
    "\n",
    "USE_PYTORCH_DDP = pytorch_setup()[0]\n",
    "\n",
    "HPARAMS = {\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"learning_rate\": 0.0017486387539278373,\n",
    "    \"one_minus_beta1\": 0.06733926164,\n",
    "    \"beta2\": 0.9955159689799007,\n",
    "    \"weight_decay\": 0.08121616522670176,\n",
    "    \"warmup_factor\": 0.02\n",
    "}\n",
    "\n",
    "\n",
    "# Modified from github.com/pytorch/pytorch/blob/v1.12.1/torch/optim/adamw.py.\n",
    "class NAdamW(torch.optim.Optimizer):\n",
    "  r\"\"\"Implements NAdamW algorithm.\n",
    "\n",
    "    See Table 1 in https://arxiv.org/abs/1910.05446 for the implementation of\n",
    "    the NAdam algorithm (there is also a comment in the code which highlights\n",
    "    the only difference of NAdamW and AdamW).\n",
    "    For further details regarding the algorithm we refer to\n",
    "    `Decoupled Weight Decay Regularization`_.\n",
    "\n",
    "    Args:\n",
    "      params (iterable): iterable of parameters to optimize or dicts defining\n",
    "          parameter groups\n",
    "      lr (float, optional): learning rate (default: 1e-3)\n",
    "      betas (Tuple[float, float], optional): coefficients used for computing\n",
    "          running averages of gradient and its square (default: (0.9, 0.999))\n",
    "      eps (float, optional): term added to the denominator to improve\n",
    "          numerical stability (default: 1e-8)\n",
    "      weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
    "    .. _Decoupled Weight Decay Regularization:\n",
    "        https://arxiv.org/abs/1711.05101\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               params,\n",
    "               lr=1e-3,\n",
    "               betas=(0.9, 0.999),\n",
    "               eps=1e-8,\n",
    "               weight_decay=1e-2):\n",
    "    if not 0.0 <= lr:\n",
    "      raise ValueError(f'Invalid learning rate: {lr}')\n",
    "    if not 0.0 <= eps:\n",
    "      raise ValueError(f'Invalid epsilon value: {eps}')\n",
    "    if not 0.0 <= betas[0] < 1.0:\n",
    "      raise ValueError(f'Invalid beta parameter at index 0: {betas[0]}')\n",
    "    if not 0.0 <= betas[1] < 1.0:\n",
    "      raise ValueError(f'Invalid beta parameter at index 1: {betas[1]}')\n",
    "    if not 0.0 <= weight_decay:\n",
    "      raise ValueError(f'Invalid weight_decay value: {weight_decay}')\n",
    "    defaults = {\n",
    "        'lr': lr, 'betas': betas, 'eps': eps, 'weight_decay': weight_decay\n",
    "    }\n",
    "    super().__init__(params, defaults)\n",
    "\n",
    "  def __setstate__(self, state):\n",
    "    super().__setstate__(state)\n",
    "    state_values = list(self.state.values())\n",
    "    step_is_tensor = (len(state_values) != 0) and torch.is_tensor(\n",
    "        state_values[0]['step'])\n",
    "    if not step_is_tensor:\n",
    "      for s in state_values:\n",
    "        s['step'] = torch.tensor(float(s['step']))\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def step(self, closure=None):\n",
    "    \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "          closure (callable, optional): A closure that reevaluates the model\n",
    "              and returns the loss.\n",
    "    \"\"\"\n",
    "    self._cuda_graph_capture_health_check()\n",
    "\n",
    "    loss = None\n",
    "    if closure is not None:\n",
    "      with torch.enable_grad():\n",
    "        loss = closure()\n",
    "\n",
    "    for group in self.param_groups:\n",
    "      params_with_grad = []\n",
    "      grads = []\n",
    "      exp_avgs = []\n",
    "      exp_avg_sqs = []\n",
    "      state_steps = []\n",
    "      beta1, beta2 = group['betas']\n",
    "\n",
    "      for p in group['params']:\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        params_with_grad.append(p)\n",
    "        if p.grad.is_sparse:\n",
    "          raise RuntimeError('NAdamW does not support sparse gradients')\n",
    "        grads.append(p.grad)\n",
    "\n",
    "        state = self.state[p]\n",
    "\n",
    "        # State initialization\n",
    "        if len(state) == 0:\n",
    "          state['step'] = torch.tensor(0.)\n",
    "          # Exponential moving average of gradient values\n",
    "          state['exp_avg'] = torch.zeros_like(\n",
    "              p, memory_format=torch.preserve_format)\n",
    "          # Exponential moving average of squared gradient values\n",
    "          state['exp_avg_sq'] = torch.zeros_like(\n",
    "              p, memory_format=torch.preserve_format)\n",
    "\n",
    "        exp_avgs.append(state['exp_avg'])\n",
    "        exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "        state_steps.append(state['step'])\n",
    "\n",
    "      nadamw(\n",
    "          params_with_grad,\n",
    "          grads,\n",
    "          exp_avgs,\n",
    "          exp_avg_sqs,\n",
    "          state_steps,\n",
    "          beta1=beta1,\n",
    "          beta2=beta2,\n",
    "          lr=group['lr'],\n",
    "          weight_decay=group['weight_decay'],\n",
    "          eps=group['eps'])\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def nadamw(params: List[Tensor],\n",
    "           grads: List[Tensor],\n",
    "           exp_avgs: List[Tensor],\n",
    "           exp_avg_sqs: List[Tensor],\n",
    "           state_steps: List[Tensor],\n",
    "           beta1: float,\n",
    "           beta2: float,\n",
    "           lr: float,\n",
    "           weight_decay: float,\n",
    "           eps: float) -> None:\n",
    "  r\"\"\"Functional API that performs NAdamW algorithm computation.\n",
    "    See NAdamW class for details.\n",
    "  \"\"\"\n",
    "\n",
    "  if not all(isinstance(t, torch.Tensor) for t in state_steps):\n",
    "    raise RuntimeError(\n",
    "        'API has changed, `state_steps` argument must contain a list of' +\n",
    "        ' singleton tensors')\n",
    "\n",
    "  for i, param in enumerate(params):\n",
    "    grad = grads[i]\n",
    "    exp_avg = exp_avgs[i]\n",
    "    exp_avg_sq = exp_avg_sqs[i]\n",
    "    step_t = state_steps[i]\n",
    "\n",
    "    # Update step.\n",
    "    step_t += 1\n",
    "\n",
    "    # Perform stepweight decay.\n",
    "    param.mul_(1 - lr * weight_decay)\n",
    "\n",
    "    # Decay the first and second moment running average coefficient.\n",
    "    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "    # Only difference between NAdamW and AdamW in this implementation.\n",
    "    # The official PyTorch implementation of NAdam uses a different algorithm.\n",
    "    # We undo these ops later on, which could cause numerical issues but saves\n",
    "    # us from having to make an extra copy of the gradients.\n",
    "    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "    step = step_t.item()\n",
    "\n",
    "    bias_correction1 = 1 - beta1**step\n",
    "    bias_correction2 = 1 - beta2**step\n",
    "\n",
    "    step_size = lr / bias_correction1\n",
    "\n",
    "    bias_correction2_sqrt = math.sqrt(bias_correction2)\n",
    "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
    "\n",
    "    param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "    exp_avg.sub_(grad, alpha=1 - beta1).div_(beta1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple test model: Single hidden layer\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup NAdamW and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data tensor([[-9.3848e-02, -4.9599e-01,  9.6214e-01,  3.9399e-01, -9.2998e-01,\n",
      "          1.1855e+00,  1.4126e+00,  1.6326e-01, -1.7673e+00, -1.3445e+00],\n",
      "        [ 1.5449e+00,  4.5563e-01,  1.1740e-01, -7.2400e-01, -2.1713e+00,\n",
      "          7.8712e-01, -6.2890e-01, -4.4137e-02, -1.2323e+00, -5.0699e-01],\n",
      "        [-1.3766e+00, -2.0966e+00, -1.0178e+00,  3.7341e-01, -1.3290e-01,\n",
      "          5.8631e-02, -1.6333e+00,  6.6700e-01, -1.8709e+00,  1.6859e-01],\n",
      "        [ 9.3317e-01,  2.0100e-01,  9.6671e-01,  2.2769e-01, -1.7471e+00,\n",
      "          2.5883e+00,  1.0854e+00,  4.4832e-02, -1.4370e+00,  1.3024e+00],\n",
      "        [ 8.5050e-01,  1.0503e+00, -1.3052e-01, -5.9629e-01, -2.8378e-01,\n",
      "         -7.7014e-01,  1.5392e+00,  3.0449e-02,  5.3290e-01,  5.7384e-01],\n",
      "        [ 9.3549e-01,  1.6361e-01, -7.3672e-01,  1.3144e+00,  9.5261e-01,\n",
      "          4.4999e-01, -1.3974e+00, -5.1735e-01,  2.9637e-01, -6.8204e-01],\n",
      "        [ 1.1491e-01,  1.1775e+00, -6.1895e-01, -5.0955e-01,  1.1652e+00,\n",
      "         -1.9671e+00,  9.0650e-01,  8.0727e-01, -4.4357e-01,  8.2577e-01],\n",
      "        [-2.7048e-01, -5.9013e-01, -7.4485e-01, -5.2122e-01, -6.6669e-01,\n",
      "         -2.2537e-01,  6.4549e-01, -4.4118e-01, -3.8494e-01,  1.0021e+00],\n",
      "        [ 9.4577e-01,  1.1957e-01, -3.6746e-01,  4.6750e-02, -7.1604e-01,\n",
      "         -6.5982e-01,  3.4781e-01,  4.0064e-01, -2.0550e+00, -3.1416e-01],\n",
      "        [ 2.7413e-01,  1.5308e+00, -1.4143e-01,  3.9775e-02,  8.7024e-02,\n",
      "          2.1354e+00,  4.5006e-01,  6.1258e-01,  2.1435e+00, -6.4505e-01],\n",
      "        [ 5.9405e-02, -2.1210e+00, -4.4447e-01,  2.9370e-01,  4.9005e-01,\n",
      "         -3.5761e-01, -5.3064e-01,  3.4932e-02,  5.9492e-02, -1.8187e+00],\n",
      "        [-6.3354e-01,  2.0197e-01, -1.4639e+00, -8.5857e-01,  7.1633e-01,\n",
      "          2.6762e-01,  7.3322e-01,  1.6809e-01,  1.3912e+00, -9.2009e-01],\n",
      "        [ 4.5433e-01,  2.6486e-01, -5.7697e-01,  6.0512e-01,  1.7202e+00,\n",
      "          4.0063e-01, -5.1369e-01,  1.0943e-01,  1.3346e-01,  9.8359e-02],\n",
      "        [ 1.8458e+00, -1.0178e+00, -3.9445e-01, -1.2492e+00,  4.9166e-01,\n",
      "          1.0738e+00,  1.7522e-01,  2.9090e-01, -4.0899e-01, -1.5271e-03],\n",
      "        [-4.4774e-01, -1.2610e+00,  8.7106e-01,  9.3889e-01, -1.3476e-01,\n",
      "          9.0448e-01, -3.0808e-01, -8.1892e-01,  1.2099e+00,  4.5584e-01],\n",
      "        [-1.0595e+00, -9.2267e-01,  4.7277e-01,  7.7532e-01,  2.3464e+00,\n",
      "          2.4255e+00, -3.2051e-01,  9.0133e-01, -5.5364e-01, -4.8775e-01],\n",
      "        [ 4.9956e-02, -6.0167e-01,  6.5802e-01,  1.4838e+00,  1.6717e+00,\n",
      "          3.9861e-01, -5.6417e-01, -1.0259e+00,  1.6735e-01, -3.1876e-01],\n",
      "        [ 3.0113e-01, -9.2452e-01, -9.4157e-01, -1.7616e+00, -3.0962e-01,\n",
      "         -2.2928e-01,  1.2253e-01, -3.2930e-02,  3.8133e-01,  1.9290e+00],\n",
      "        [-4.7556e-01,  5.3799e-01,  1.0368e+00,  6.4829e-01, -4.6472e-01,\n",
      "          1.3381e+00,  1.1592e-01, -3.3547e-01,  1.5495e+00, -8.0255e-01],\n",
      "        [-1.2325e+00,  7.4472e-01,  1.7153e+00,  1.2799e+00, -8.7376e-01,\n",
      "         -1.5793e+00,  3.6534e-01,  2.0595e+00, -1.2339e+00,  2.5179e-01]])\n",
      "target_data tensor([[ 1.0125],\n",
      "        [-1.2802],\n",
      "        [-0.5008],\n",
      "        [ 1.6187],\n",
      "        [ 0.0172],\n",
      "        [-0.5525],\n",
      "        [-0.9165],\n",
      "        [ 0.0535],\n",
      "        [-0.0504],\n",
      "        [ 0.4252],\n",
      "        [-0.8864],\n",
      "        [ 0.5954],\n",
      "        [-0.8343],\n",
      "        [ 0.4233],\n",
      "        [-0.1747],\n",
      "        [ 0.8627],\n",
      "        [ 1.1140],\n",
      "        [-0.5515],\n",
      "        [-1.2283],\n",
      "        [ 1.3539]])\n",
      "Compare the parameters before optimization\n",
      "Comparing fc1.weight:\n",
      "Model 1: tensor([[-0.0757,  0.2381, -0.0717, -0.0222,  0.0698, -0.0998, -0.1828,  0.2091,\n",
      "         -0.0435, -0.0088],\n",
      "        [ 0.1335, -0.1077,  0.1319,  0.0467, -0.0774, -0.0588,  0.0831, -0.0726,\n",
      "         -0.1297,  0.2938],\n",
      "        [-0.0397,  0.2879,  0.1833,  0.2081,  0.0738,  0.0280, -0.0825,  0.0311,\n",
      "          0.3019, -0.2423],\n",
      "        [ 0.2063,  0.1705, -0.2401,  0.2387,  0.0450,  0.1177,  0.0162,  0.0670,\n",
      "         -0.0893,  0.0467],\n",
      "        [ 0.1238, -0.0010,  0.3151,  0.3121, -0.1154, -0.2065, -0.0883, -0.1074,\n",
      "         -0.2473, -0.0752]], grad_fn=<CloneBackward0>)\n",
      "Model 2: tensor([[-0.0757,  0.2381, -0.0717, -0.0222,  0.0698, -0.0998, -0.1828,  0.2091,\n",
      "         -0.0435, -0.0088],\n",
      "        [ 0.1335, -0.1077,  0.1319,  0.0467, -0.0774, -0.0588,  0.0831, -0.0726,\n",
      "         -0.1297,  0.2938],\n",
      "        [-0.0397,  0.2879,  0.1833,  0.2081,  0.0738,  0.0280, -0.0825,  0.0311,\n",
      "          0.3019, -0.2423],\n",
      "        [ 0.2063,  0.1705, -0.2401,  0.2387,  0.0450,  0.1177,  0.0162,  0.0670,\n",
      "         -0.0893,  0.0467],\n",
      "        [ 0.1238, -0.0010,  0.3151,  0.3121, -0.1154, -0.2065, -0.0883, -0.1074,\n",
      "         -0.2473, -0.0752]], grad_fn=<CloneBackward0>)\n",
      "Difference: 0.0\n",
      "Comparing fc1.bias:\n",
      "Model 1: tensor([ 0.0234,  0.1575, -0.3025,  0.2698, -0.1240], grad_fn=<CloneBackward0>)\n",
      "Model 2: tensor([ 0.0234,  0.1575, -0.3025,  0.2698, -0.1240], grad_fn=<CloneBackward0>)\n",
      "Difference: 0.0\n",
      "Comparing fc2.weight:\n",
      "Model 1: tensor([[ 0.4382, -0.3315,  0.1789,  0.4024, -0.2535]],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "Model 2: tensor([[ 0.4382, -0.3315,  0.1789,  0.4024, -0.2535]],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "Difference: 0.0\n",
      "Comparing fc2.bias:\n",
      "Model 1: tensor([0.2102], grad_fn=<CloneBackward0>)\n",
      "Model 2: tensor([0.2102], grad_fn=<CloneBackward0>)\n",
      "Difference: 0.0\n",
      "Compare the parameters after optimization\n",
      "Comparing fc1.weight:\n",
      "Model 1: tensor([[-0.0947,  0.2191, -0.0527, -0.0412,  0.0508, -0.0807, -0.1637,  0.2281,\n",
      "         -0.0245, -0.0278],\n",
      "        [ 0.1525, -0.0887,  0.1129,  0.0277, -0.0584, -0.0778,  0.0641, -0.0916,\n",
      "         -0.1106,  0.3127],\n",
      "        [-0.0587,  0.2689,  0.2022,  0.2271,  0.0547,  0.0090, -0.0635,  0.0501,\n",
      "          0.2829, -0.2233],\n",
      "        [ 0.1873,  0.1515, -0.2211,  0.2577,  0.0260,  0.1367,  0.0352,  0.0860,\n",
      "         -0.1083,  0.0657],\n",
      "        [ 0.1428, -0.0200,  0.2961,  0.2931, -0.0964, -0.2255, -0.1073, -0.1264,\n",
      "         -0.2283, -0.0942]], grad_fn=<CloneBackward0>)\n",
      "Model 2: tensor([[-0.1137,  0.2001, -0.0337, -0.0602,  0.0318, -0.0617, -0.1447,  0.2470,\n",
      "         -0.0055, -0.0468],\n",
      "        [ 0.1715, -0.0697,  0.0939,  0.0087, -0.0394, -0.0968,  0.0451, -0.1106,\n",
      "         -0.0916,  0.3317],\n",
      "        [-0.0777,  0.2498,  0.2212,  0.2461,  0.0357, -0.0100, -0.0445,  0.0691,\n",
      "          0.2638, -0.2042],\n",
      "        [ 0.1683,  0.1325, -0.2021,  0.2767,  0.0069,  0.1557,  0.0542,  0.1050,\n",
      "         -0.1273,  0.0847],\n",
      "        [ 0.1617, -0.0390,  0.2770,  0.2741, -0.0774, -0.2445, -0.1262, -0.1453,\n",
      "         -0.2093, -0.1132]], grad_fn=<CloneBackward0>)\n",
      "Difference: 0.13437053561210632\n",
      "Comparing fc1.bias:\n",
      "Model 1: tensor([ 0.0044,  0.1764, -0.3215,  0.2508, -0.1049], grad_fn=<CloneBackward0>)\n",
      "Model 2: tensor([-0.0146,  0.1954, -0.3405,  0.2318, -0.0859], grad_fn=<CloneBackward0>)\n",
      "Difference: 0.042483363300561905\n",
      "Comparing fc2.weight:\n",
      "Model 1: tensor([[ 0.4191, -0.3125,  0.1599,  0.3833, -0.2345]],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "Model 2: tensor([[ 0.4001, -0.2934,  0.1408,  0.3643, -0.2155]],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "Difference: 0.042556993663311005\n",
      "Comparing fc2.bias:\n",
      "Model 1: tensor([0.1912], grad_fn=<CloneBackward0>)\n",
      "Model 2: tensor([0.1722], grad_fn=<CloneBackward0>)\n",
      "Difference: 0.01902100443840027\n",
      "Compare the steps taken by the two optimizers\n",
      "Comparing fc1.weight:\n",
      "Model 1: tensor([[-0.0190, -0.0190,  0.0190, -0.0190, -0.0190,  0.0190,  0.0190,  0.0190,\n",
      "          0.0190, -0.0190],\n",
      "        [ 0.0190,  0.0190, -0.0190, -0.0190,  0.0190, -0.0190, -0.0190, -0.0190,\n",
      "          0.0190,  0.0190],\n",
      "        [-0.0190, -0.0190,  0.0190,  0.0190, -0.0190, -0.0190,  0.0190,  0.0190,\n",
      "         -0.0190,  0.0190],\n",
      "        [-0.0190, -0.0190,  0.0190,  0.0190, -0.0190,  0.0190,  0.0190,  0.0190,\n",
      "         -0.0190,  0.0190],\n",
      "        [ 0.0190, -0.0190, -0.0190, -0.0190,  0.0190, -0.0190, -0.0190, -0.0190,\n",
      "          0.0190, -0.0190]], grad_fn=<SubBackward0>)\n",
      "Model 2: tensor([[-0.0380, -0.0380,  0.0380, -0.0380, -0.0380,  0.0380,  0.0380,  0.0380,\n",
      "          0.0380, -0.0380],\n",
      "        [ 0.0380,  0.0380, -0.0380, -0.0380,  0.0380, -0.0380, -0.0380, -0.0380,\n",
      "          0.0380,  0.0379],\n",
      "        [-0.0380, -0.0381,  0.0380,  0.0380, -0.0380, -0.0380,  0.0380,  0.0380,\n",
      "         -0.0381,  0.0380],\n",
      "        [-0.0380, -0.0380,  0.0380,  0.0380, -0.0380,  0.0380,  0.0380,  0.0380,\n",
      "         -0.0380,  0.0380],\n",
      "        [ 0.0380, -0.0380, -0.0381, -0.0381,  0.0380, -0.0380, -0.0380, -0.0380,\n",
      "          0.0380, -0.0380]], grad_fn=<SubBackward0>)\n",
      "Norm of Difference: 0.13437053561210632\n",
      "Difference for each individual param: tensor([[ 0.0190,  0.0190, -0.0190,  0.0190,  0.0190, -0.0190, -0.0190, -0.0190,\n",
      "         -0.0190,  0.0190],\n",
      "        [-0.0190, -0.0190,  0.0190,  0.0190, -0.0190,  0.0190,  0.0190,  0.0190,\n",
      "         -0.0190, -0.0190],\n",
      "        [ 0.0190,  0.0190, -0.0190, -0.0190,  0.0190,  0.0190, -0.0190, -0.0190,\n",
      "          0.0190, -0.0190],\n",
      "        [ 0.0190,  0.0190, -0.0190, -0.0190,  0.0190, -0.0190, -0.0190, -0.0190,\n",
      "          0.0190, -0.0190],\n",
      "        [-0.0190,  0.0190,  0.0190,  0.0190, -0.0190,  0.0190,  0.0190,  0.0190,\n",
      "         -0.0190,  0.0190]], grad_fn=<SubBackward0>)\n",
      "Ratio for each individual param: tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000]], grad_fn=<DivBackward0>)\n",
      "Weight 0: Difference = 0.018992431461811066, Ratio = 0.49999991059303284, Single precision not exceeded (+- e-7): True\n",
      "Weight 1: Difference = 0.019023790955543518, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Weight 2: Difference = -0.019007164984941483, Ratio = 0.5000001192092896, Single precision not exceeded (+- e-7): True\n",
      "Weight 3: Difference = 0.01899777166545391, Ratio = 0.4999999403953552, Single precision not exceeded (+- e-7): True\n",
      "Weight 4: Difference = 0.019006971269845963, Ratio = 0.5000001192092896, Single precision not exceeded (+- e-7): True\n",
      "Weight 5: Difference = -0.01900995522737503, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Weight 6: Difference = -0.019018277525901794, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 7: Difference = -0.018979087471961975, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 8: Difference = -0.019004210829734802, Ratio = 0.5000001192092896, Single precision not exceeded (+- e-7): True\n",
      "Weight 9: Difference = 0.018999114632606506, Ratio = 0.4999999403953552, Single precision not exceeded (+- e-7): True\n",
      "Weight 10: Difference = -0.018986642360687256, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 11: Difference = -0.019010700285434723, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Weight 12: Difference = 0.019013188779354095, Ratio = 0.5000001192092896, Single precision not exceeded (+- e-7): True\n",
      "Weight 13: Difference = 0.019004665315151215, Ratio = 0.5000000596046448, Single precision not exceeded (+- e-7): True\n",
      "Weight 14: Difference = -0.019007734954357147, Ratio = 0.5000001192092896, Single precision not exceeded (+- e-7): True\n",
      "Weight 15: Difference = 0.01899411529302597, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 16: Difference = 0.019008301198482513, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Weight 17: Difference = 0.018992729485034943, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 18: Difference = -0.01901295781135559, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 19: Difference = -0.018970638513565063, Ratio = 0.49999961256980896, Single precision not exceeded (+- e-7): True\n",
      "Weight 20: Difference = 0.018996022641658783, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 21: Difference = 0.019028782844543457, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 22: Difference = -0.018981680274009705, Ratio = 0.4999997913837433, Single precision not exceeded (+- e-7): True\n",
      "Weight 23: Difference = -0.018979132175445557, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 24: Difference = 0.019007284194231033, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 25: Difference = 0.019002797082066536, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 26: Difference = -0.019008241593837738, Ratio = 0.5000001192092896, Single precision not exceeded (+- e-7): True\n",
      "Weight 27: Difference = -0.018996888771653175, Ratio = 0.4999999403953552, Single precision not exceeded (+- e-7): True\n",
      "Weight 28: Difference = 0.01903018355369568, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 29: Difference = -0.019024208188056946, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Weight 30: Difference = 0.01902061700820923, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Weight 31: Difference = 0.01901702582836151, Ratio = 0.5000004172325134, Single precision not exceeded (+- e-7): True\n",
      "Weight 32: Difference = -0.019023984670639038, Ratio = 0.5000004172325134, Single precision not exceeded (+- e-7): True\n",
      "Weight 33: Difference = -0.0189761221408844, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Weight 34: Difference = 0.019004490226507187, Ratio = 0.5000000596046448, Single precision not exceeded (+- e-7): True\n",
      "Weight 35: Difference = -0.018988221883773804, Ratio = 0.5000001192092896, Single precision not exceeded (+- e-7): True\n",
      "Weight 36: Difference = -0.01899838075041771, Ratio = 0.4999999403953552, Single precision not exceeded (+- e-7): True\n",
      "Weight 37: Difference = -0.018993303179740906, Ratio = 0.49999991059303284, Single precision not exceeded (+- e-7): True\n",
      "Weight 38: Difference = 0.018991075456142426, Ratio = 0.4999997913837433, Single precision not exceeded (+- e-7): True\n",
      "Weight 39: Difference = -0.01899532973766327, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 40: Difference = -0.01898762583732605, Ratio = 0.49999991059303284, Single precision not exceeded (+- e-7): True\n",
      "Weight 41: Difference = 0.01899990253150463, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 42: Difference = 0.019031494855880737, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 43: Difference = 0.019031167030334473, Ratio = 0.5000007748603821, Single precision not exceeded (+- e-7): True\n",
      "Weight 44: Difference = -0.01901153475046158, Ratio = 0.5000001192092896, Single precision not exceeded (+- e-7): True\n",
      "Weight 45: Difference = 0.01897934079170227, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 46: Difference = 0.018991172313690186, Ratio = 0.49999991059303284, Single precision not exceeded (+- e-7): True\n",
      "Weight 47: Difference = 0.018989264965057373, Ratio = 0.49999991059303284, Single precision not exceeded (+- e-7): True\n",
      "Weight 48: Difference = -0.019024714827537537, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Weight 49: Difference = 0.018992476165294647, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Comparing fc1.bias:\n",
      "Model 1: tensor([-0.0190,  0.0190, -0.0190, -0.0190,  0.0190], grad_fn=<SubBackward0>)\n",
      "Model 2: tensor([-0.0380,  0.0380, -0.0379, -0.0381,  0.0380], grad_fn=<SubBackward0>)\n",
      "Norm of Difference: 0.042483363300561905\n",
      "Difference for each individual param: tensor([ 0.0190, -0.0190,  0.0190,  0.0190, -0.0190], grad_fn=<SubBackward0>)\n",
      "Ratio for each individual param: tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<DivBackward0>)\n",
      "Weight 0: Difference = 0.019002335146069527, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 1: Difference = -0.01898425817489624, Ratio = 0.49999961256980896, Single precision not exceeded (+- e-7): True\n",
      "Weight 2: Difference = 0.018969744443893433, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 3: Difference = 0.01902693510055542, Ratio = 0.5000007748603821, Single precision not exceeded (+- e-7): True\n",
      "Weight 4: Difference = -0.019012361764907837, Ratio = 0.5000001788139343, Single precision not exceeded (+- e-7): True\n",
      "Comparing fc2.weight:\n",
      "Model 1: tensor([[-0.0190,  0.0190, -0.0190, -0.0190,  0.0190]], grad_fn=<SubBackward0>)\n",
      "Model 2: tensor([[-0.0381,  0.0381, -0.0380, -0.0381,  0.0381]], grad_fn=<SubBackward0>)\n",
      "Norm of Difference: 0.042556993663311005\n",
      "Difference for each individual param: tensor([[ 0.0190, -0.0190,  0.0190,  0.0190, -0.0190]], grad_fn=<SubBackward0>)\n",
      "Ratio for each individual param: tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], grad_fn=<DivBackward0>)\n",
      "Weight 0: Difference = 0.019043773412704468, Ratio = 0.5000007748603821, Single precision not exceeded (+- e-7): True\n",
      "Weight 1: Difference = -0.019033104181289673, Ratio = 0.5000007748603821, Single precision not exceeded (+- e-7): True\n",
      "Weight 2: Difference = 0.019017890095710754, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 3: Difference = 0.019040226936340332, Ratio = 0.5, Single precision not exceeded (+- e-7): True\n",
      "Weight 4: Difference = -0.019025325775146484, Ratio = 0.5000004172325134, Single precision not exceeded (+- e-7): True\n",
      "Comparing fc2.bias:\n",
      "Model 1: tensor([-0.0190], grad_fn=<SubBackward0>)\n",
      "Model 2: tensor([-0.0380], grad_fn=<SubBackward0>)\n",
      "Norm of Difference: 0.01902100443840027\n",
      "Difference for each individual param: tensor([0.0190], grad_fn=<SubBackward0>)\n",
      "Ratio for each individual param: tensor([0.5000], grad_fn=<DivBackward0>)\n",
      "Weight 0: Difference = 0.01902100443840027, Ratio = 0.5000004172325134, Single precision not exceeded (+- e-7): True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize two models with the same parameters\n",
    "model1 = SimpleNet()\n",
    "model2 = SimpleNet()\n",
    "\n",
    "# Make sure the models are initialized with the same weights\n",
    "model2.load_state_dict(model1.state_dict())\n",
    "\n",
    "lr = 0.01\n",
    "# Define the optimizer with a regular learning rate for model1\n",
    "optimizer1 = NAdamW(model1.parameters(), lr=lr)\n",
    "\n",
    "# Define the optimizer with a doubled learning rate for model2\n",
    "optimizer2 = NAdamW(model2.parameters(), lr=lr*2)\n",
    "\n",
    "# save the initialo parameters for comparison\n",
    "params1_before = {name: param.clone() for name, param in model1.named_parameters()}\n",
    "params2_before = {name: param.clone() for name, param in model2.named_parameters()}\n",
    "\n",
    "# Create some dummy input and target data\n",
    "torch.manual_seed(37)\n",
    "input_data = torch.randn(20, 10)  # Batch size of 20, 10 features\n",
    "target_data = torch.randn(20, 1)  # Target for regression\n",
    "print(\"input_data\", input_data)\n",
    "print(\"target_data\", target_data)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Forward and backward pass for model 1\n",
    "output1 = model1(input_data)\n",
    "loss1 = loss_fn(output1, target_data)\n",
    "optimizer1.zero_grad()\n",
    "loss1.backward()\n",
    "optimizer1.step()\n",
    "\n",
    "# Save the new parameters for comparison\n",
    "params1_after = {name: param.clone() for name, param in model1.named_parameters()}\n",
    "\n",
    "# Forward and backward pass for model 2 (same data)\n",
    "output2 = model2(input_data)\n",
    "loss2 = loss_fn(output2, target_data)\n",
    "optimizer2.zero_grad()\n",
    "loss2.backward()\n",
    "optimizer2.step()\n",
    "\n",
    "print(\"Compare the parameters before optimization\")\n",
    "# Save the new parameters for comparison\n",
    "params2_after = {name: param.clone() for name, param in model2.named_parameters()}\n",
    "# Now, compare the parameters before optimization\n",
    "for (name1, param1), (name2, param2) in zip(params1_before.items(), params2_before.items()):\n",
    "    print(f\"Comparing {name1}:\")\n",
    "    print(f\"Model 1: {param1}\")\n",
    "    print(f\"Model 2: {param2}\")\n",
    "    print(f\"Difference: {torch.norm(param1 - param2)}\")\n",
    "\n",
    "print(\"Compare the parameters after optimization\")\n",
    "# Now, compare the parameters after optimization\n",
    "for (name1, param1), (name2, param2) in zip(params1_after.items(), params2_after.items()):\n",
    "    print(f\"Comparing {name1}:\")\n",
    "    print(f\"Model 1: {param1}\")\n",
    "    print(f\"Model 2: {param2}\")\n",
    "    print(f\"Difference: {torch.norm(param1 - param2)}\")\n",
    "\n",
    "# extract the step that was taken by calculating the difference between the parameters before and after optimization\n",
    "step1 = {name: params1_after[name] - params1_before[name] for name in params1_after}\n",
    "step2 = {name: params2_after[name] - params2_before[name] for name in params2_after}\n",
    "\n",
    "print(\"Compare the steps taken by the two optimizers\")\n",
    "\n",
    "for (name1, param1), (name2, param2) in zip(step1.items(), step2.items()):\n",
    "    print(f\"Comparing {name1}:\")\n",
    "    \n",
    "    # Print the full parameter tensors for both models (optional)\n",
    "    print(f\"Model 1: {param1}\")\n",
    "    print(f\"Model 2: {param2}\")\n",
    "    \n",
    "    # Print the norm of the difference (a single value, scalar)\n",
    "    print(f\"Norm of Difference: {torch.norm(param1 - param2)}\")\n",
    "    \n",
    "    # Compute the difference and print for each individual parameter\n",
    "    difference = param1 - param2\n",
    "    print(f\"Difference for each individual param: {difference}\")\n",
    "    \n",
    "    # Compute the ratio and print for each individual parameter\n",
    "    ratio = param1 / param2\n",
    "    print(f\"Ratio for each individual param: {ratio}\")\n",
    "    \n",
    "    # Optionally, print the individual weight differences and ratios clearly\n",
    "    for i, (diff, r) in enumerate(zip(difference.flatten(), ratio.flatten())):\n",
    "        print(f\"Weight {i}: Difference = {diff.item()}, Ratio = {r.item()}, Single precision not exceeded (+- lr* e-7): {torch.isclose(r, torch.tensor(0.5), atol=lr * 1e-7)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as per this data, the learning rate scales the step the optimizer suggests linearly "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
