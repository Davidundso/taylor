{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4f5d55",
   "metadata": {},
   "source": [
    "I will try to explain and do the following steps:\n",
    "Load Data\n",
    "Define PyToch Model\n",
    "Define Loss Function and Optimizers\n",
    "Run a Training Loop\n",
    "Evaluate the Model\n",
    "Make Predictions\n",
    "\n",
    "### (1) Load Data\n",
    "\n",
    "First the following imports are needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf998bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe57cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (768, 8), y shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6012b95",
   "metadata": {},
   "source": [
    "PyTorch works with the \"tensor\" data type, one should convert e.g. bc NumPy uses 64 bit floats and PyTorch uses 32 bit floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43e303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32) # convert to tensor\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b2eeb",
   "metadata": {},
   "source": [
    "The reshape operation changes y from a 1D to a 2D array:\n",
    "befor y is 1D: y = [0, 1, 1, 0, 1] \n",
    "\n",
    "after, each element has its own row: \n",
    "y = [[0], [1],\n",
    "     [1],\n",
    "     [0],\n",
    "     [1]]\n",
    "     \n",
    "This is done because PyTorch expects/prefers this format.\n",
    "\n",
    "### (2) Define the model\n",
    "\n",
    "The standart way to do this is by writing a class that inherits from the torch.nn.Module and then defining the layers sequentially. \n",
    "Of course: The input layer must fit the dimension of the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19234936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PimaClassifier(nn.Module):          # inherit from nn.Module\n",
    "    def __init__(self):                   \n",
    "        super().__init__()                # call the parents` class constructor (necessary!)\n",
    "        self.hidden1 = nn.Linear(8, 12)   # Linear Layer: 8 inputs, 12 outputs\n",
    "        self.act1 = nn.ReLU()             # activation function for the layer: ReLu\n",
    "        self.hidden2 = nn.Linear(12, 8)   # 2nd layer: 12 in, 8 out\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)     # Output layer: In this case (classification) one output neuron\n",
    "        self.act_output = nn.Sigmoid()    # Sigmoid fct for classification problem\n",
    "\n",
    "    def forward(self, x):                 # define how the data is passed forward through the network\n",
    "        x = self.act1(self.hidden1(x))    # Verkettung: Lineare fkt innen, Aktivierungsfkt au√üen\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = PimaClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a2fe8",
   "metadata": {},
   "source": [
    "### (3) Preparation for training \n",
    "define loss function (binary classification = binary cross entropy loss) \n",
    "choose optimizer (standart = adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee06f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # adam probably has more hyperparams than the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38dffe",
   "metadata": {},
   "source": [
    "### (4) Training the model  \n",
    "\n",
    "Epoch: Passes the entire training dataset to the model once \n",
    "\n",
    "Batch: One or more samples passed to the model, from which the gradient descent algorithm will be executed for one iteration (batch size linearly in relation to the number of computations) \n",
    "\n",
    "Pass batches (whole data in batches) in loops (epochs) through the model until satisfied with the models output. \n",
    " \n",
    " The simplest way to build a training loop is to use two nested for-loops, one for epochs and one for batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c88b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 0.35489439964294434\n",
      "Finished epoch 1, latest loss 0.35613521933555603\n",
      "Finished epoch 2, latest loss 0.3527310788631439\n",
      "Finished epoch 3, latest loss 0.3503945469856262\n",
      "Finished epoch 4, latest loss 0.3497575521469116\n",
      "Finished epoch 5, latest loss 0.34929901361465454\n",
      "Finished epoch 6, latest loss 0.34627434611320496\n",
      "Finished epoch 7, latest loss 0.3496924042701721\n",
      "Finished epoch 8, latest loss 0.34492576122283936\n",
      "Finished epoch 9, latest loss 0.34368741512298584\n",
      "Finished epoch 10, latest loss 0.3419565260410309\n",
      "Finished epoch 11, latest loss 0.3382900059223175\n",
      "Finished epoch 12, latest loss 0.3406706154346466\n",
      "Finished epoch 13, latest loss 0.3387353718280792\n",
      "Finished epoch 14, latest loss 0.3368164598941803\n",
      "Finished epoch 15, latest loss 0.334529310464859\n",
      "Finished epoch 16, latest loss 0.331054151058197\n",
      "Finished epoch 17, latest loss 0.32856544852256775\n",
      "Finished epoch 18, latest loss 0.3387570083141327\n",
      "Finished epoch 19, latest loss 0.33152952790260315\n",
      "Finished epoch 20, latest loss 0.32547104358673096\n",
      "Finished epoch 21, latest loss 0.3323855698108673\n",
      "Finished epoch 22, latest loss 0.33504718542099\n",
      "Finished epoch 23, latest loss 0.3244139552116394\n",
      "Finished epoch 24, latest loss 0.3182663917541504\n",
      "Finished epoch 25, latest loss 0.3195370137691498\n",
      "Finished epoch 26, latest loss 0.3205515146255493\n",
      "Finished epoch 27, latest loss 0.3256312310695648\n",
      "Finished epoch 28, latest loss 0.31900352239608765\n",
      "Finished epoch 29, latest loss 0.31833958625793457\n",
      "Finished epoch 30, latest loss 0.31769174337387085\n",
      "Finished epoch 31, latest loss 0.31669044494628906\n",
      "Finished epoch 32, latest loss 0.32086196541786194\n",
      "Finished epoch 33, latest loss 0.31624430418014526\n",
      "Finished epoch 34, latest loss 0.31078267097473145\n",
      "Finished epoch 35, latest loss 0.3080289363861084\n",
      "Finished epoch 36, latest loss 0.3037712574005127\n",
      "Finished epoch 37, latest loss 0.29737573862075806\n",
      "Finished epoch 38, latest loss 0.3118833601474762\n",
      "Finished epoch 39, latest loss 0.3048739433288574\n",
      "Finished epoch 40, latest loss 0.31178849935531616\n",
      "Finished epoch 41, latest loss 0.2931533455848694\n",
      "Finished epoch 42, latest loss 0.30545443296432495\n",
      "Finished epoch 43, latest loss 0.2971227765083313\n",
      "Finished epoch 44, latest loss 0.2988593876361847\n",
      "Finished epoch 45, latest loss 0.2916772663593292\n",
      "Finished epoch 46, latest loss 0.29419878125190735\n",
      "Finished epoch 47, latest loss 0.3024328351020813\n",
      "Finished epoch 48, latest loss 0.2847039997577667\n",
      "Finished epoch 49, latest loss 0.2847367227077484\n",
      "Finished epoch 50, latest loss 0.29036104679107666\n",
      "Finished epoch 51, latest loss 0.2874690592288971\n",
      "Finished epoch 52, latest loss 0.2868448495864868\n",
      "Finished epoch 53, latest loss 0.28224605321884155\n",
      "Finished epoch 54, latest loss 0.28104904294013977\n",
      "Finished epoch 55, latest loss 0.28146830201148987\n",
      "Finished epoch 56, latest loss 0.28467807173728943\n",
      "Finished epoch 57, latest loss 0.2985788583755493\n",
      "Finished epoch 58, latest loss 0.28558409214019775\n",
      "Finished epoch 59, latest loss 0.2865666151046753\n",
      "Finished epoch 60, latest loss 0.28912076354026794\n",
      "Finished epoch 61, latest loss 0.2796397805213928\n",
      "Finished epoch 62, latest loss 0.27952688932418823\n",
      "Finished epoch 63, latest loss 0.276580274105072\n",
      "Finished epoch 64, latest loss 0.2861621677875519\n",
      "Finished epoch 65, latest loss 0.27979305386543274\n",
      "Finished epoch 66, latest loss 0.2793651819229126\n",
      "Finished epoch 67, latest loss 0.2873862683773041\n",
      "Finished epoch 68, latest loss 0.2729703187942505\n",
      "Finished epoch 69, latest loss 0.2784821093082428\n",
      "Finished epoch 70, latest loss 0.27046477794647217\n",
      "Finished epoch 71, latest loss 0.27416127920150757\n",
      "Finished epoch 72, latest loss 0.2778955101966858\n",
      "Finished epoch 73, latest loss 0.2753852605819702\n",
      "Finished epoch 74, latest loss 0.2760563790798187\n",
      "Finished epoch 75, latest loss 0.27432429790496826\n",
      "Finished epoch 76, latest loss 0.27077075839042664\n",
      "Finished epoch 77, latest loss 0.2785971760749817\n",
      "Finished epoch 78, latest loss 0.2722049355506897\n",
      "Finished epoch 79, latest loss 0.2680281698703766\n",
      "Finished epoch 80, latest loss 0.27741947770118713\n",
      "Finished epoch 81, latest loss 0.2687208950519562\n",
      "Finished epoch 82, latest loss 0.27328985929489136\n",
      "Finished epoch 83, latest loss 0.2700447738170624\n",
      "Finished epoch 84, latest loss 0.26646655797958374\n",
      "Finished epoch 85, latest loss 0.2718850374221802\n",
      "Finished epoch 86, latest loss 0.2695696949958801\n",
      "Finished epoch 87, latest loss 0.2638743817806244\n",
      "Finished epoch 88, latest loss 0.2636980712413788\n",
      "Finished epoch 89, latest loss 0.2640501856803894\n",
      "Finished epoch 90, latest loss 0.2717105746269226\n",
      "Finished epoch 91, latest loss 0.25534698367118835\n",
      "Finished epoch 92, latest loss 0.26433590054512024\n",
      "Finished epoch 93, latest loss 0.26124170422554016\n",
      "Finished epoch 94, latest loss 0.26118364930152893\n",
      "Finished epoch 95, latest loss 0.26996299624443054\n",
      "Finished epoch 96, latest loss 0.26201173663139343\n",
      "Finished epoch 97, latest loss 0.25331491231918335\n",
      "Finished epoch 98, latest loss 0.26269182562828064\n",
      "Finished epoch 99, latest loss 0.26396727561950684\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):               # Loop over the number of epochs\n",
    "    # Loop over the dataset in batches (batch gradient descent\n",
    "    for i in range(0, len(X), batch_size):  # range(start idx, end idx (stops one before), step size)\n",
    "        Xbatch = X[i:i+batch_size]          # Get a batch of input data (Xbatch) of size 'batch_size'\n",
    "        y_pred = model(Xbatch)              # Forward pass: compute the model's predictions for the current batch\n",
    "        ybatch = y[i:i+batch_size]          # Get the corresponding batch of target/output data (ybatch)\n",
    "        loss = loss_fn(y_pred, ybatch)      # Calculate the loss between the predictions (y_pred) and the actual targets (ybatch)\n",
    "        optimizer.zero_grad()               # Zero the gradients from the previous iteration (necessary before performing backprop)\n",
    "        loss.backward()                     # Backward pass: compute the gradients of the loss with respect to the model's parameters\n",
    "        optimizer.step()                    # Update the model's parameters using the gradients and the optimizer's learning rate\n",
    "\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790db53",
   "metadata": {},
   "source": [
    "### (5) Evaluate the Model \n",
    "\n",
    "This training loop only uses one training set. Of course normally we have: Training set (80%), Validation set (10%) and Testset (10%) to get a real performance prediction. Here, we can only evaluate performance on the training data (so, do it the same way but on the testset) \n",
    "Reminder: **Accuracy** measures the proportion of correct predictions out of the total number of predictions:\n",
    "\n",
    "$ \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6baf0ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7955729365348816\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy (no_grad is optional)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dab386",
   "metadata": {},
   "source": [
    "The round() function rounds off the floating point to the nearest integer. The == operator compares and returns a Boolean tensor, which can be converted to floating point numbers 1.0 and 0.0. The mean() function will provide you the count of the number of 1‚Äôs (i.e., prediction matches the label) divided by the total number of samples. The no_grad() context is optional but suggested, so you relieve y_pred from remembering how it comes up with the number since you are not going to do differentiation on it. \n",
    "One could do the whole training a few times to see how different models perform (stochastic process!) \n",
    " \n",
    " ### (6) Make predictions \n",
    " We can now use the model to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e86078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# make class predictions with the model\n",
    "predictions = (model(X) > 0.5).int()\n",
    "for i in range(5):\n",
    "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f6be8",
   "metadata": {},
   "source": [
    "### Summary \n",
    "You discovered how to create your first neural network model using PyTorch. Specifically, you learned the key steps in using PyTorch to create a neural network or deep learning model step by step, including:\n",
    "\n",
    "How to load data\n",
    "How to define a neural network in PyTorch\n",
    "How to train a model on data\n",
    "How to evaluate a model\n",
    "How to make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f33309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
