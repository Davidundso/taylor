{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Example using all extensions\n",
    "\n",
    "Basic example showing how compute the gradient,\n",
    "and and other quantities with BackPACK,\n",
    "on a linear model for MNIST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading some dummy data and extending the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'backpack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rand\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEntropyLoss, Flatten, Linear, Sequential\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbackpack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backpack, extend\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbackpack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     GGNMP,\n\u001b[1;32m      7\u001b[0m     HMP,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     Variance,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbackpack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_one_batch_mnist\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'backpack'"
     ]
    }
   ],
   "source": [
    "from torch import rand\n",
    "from torch.nn import CrossEntropyLoss, Flatten, Linear, Sequential\n",
    "\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import (\n",
    "    GGNMP,\n",
    "    HMP,\n",
    "    KFAC,\n",
    "    KFLR,\n",
    "    KFRA,\n",
    "    PCHMP,\n",
    "    BatchDiagGGNExact,\n",
    "    BatchDiagGGNMC,\n",
    "    BatchDiagHessian,\n",
    "    BatchGrad,\n",
    "    BatchL2Grad,\n",
    "    DiagGGNExact,\n",
    "    DiagGGNMC,\n",
    "    DiagHessian,\n",
    "    SqrtGGNExact,\n",
    "    SqrtGGNMC,\n",
    "    SumGradSquared,\n",
    "    Variance,\n",
    ")\n",
    "from backpack.utils.examples import load_one_batch_mnist\n",
    "\n",
    "X, y = load_one_batch_mnist(batch_size=512)\n",
    "\n",
    "model = Sequential(Flatten(), Linear(784, 10))\n",
    "lossfunc = CrossEntropyLoss()\n",
    "\n",
    "model = extend(model)\n",
    "lossfunc = extend(lossfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First order extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(BatchGrad()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".grad_batch.shape:       \", param.grad_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(Variance()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".variance.shape:         \", param.variance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second moment/sum of gradients squared\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(SumGradSquared()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".sum_grad_squared.shape: \", param.sum_grad_squared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 norm of individual gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(BatchL2Grad()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".batch_l2.shape:         \", param.batch_l2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to ask for multiple quantities at once\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(BatchGrad(), Variance(), SumGradSquared(), BatchL2Grad()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".grad_batch.shape:       \", param.grad_batch.shape)\n",
    "    print(\".variance.shape:         \", param.variance.shape)\n",
    "    print(\".sum_grad_squared.shape: \", param.sum_grad_squared.shape)\n",
    "    print(\".batch_l2.shape:         \", param.batch_l2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonal of the generalized Gauss-Newton and its Monte-Carlo approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(DiagGGNExact(), DiagGGNMC(mc_samples=1)):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".diag_ggn_mc.shape:      \", param.diag_ggn_mc.shape)\n",
    "    print(\".diag_ggn_exact.shape:   \", param.diag_ggn_exact.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per-sample diagonal of the generalized Gauss-Newton and its Monte-Carlo approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(BatchDiagGGNExact(), BatchDiagGGNMC(mc_samples=1)):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".diag_ggn_mc_batch.shape:      \", param.diag_ggn_mc_batch.shape)\n",
    "    print(\".diag_ggn_exact_batch.shape:   \", param.diag_ggn_exact_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFAC, KFRA and KFLR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(KFAC(mc_samples=1), KFLR(), KFRA()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".kfac (shapes):          \", [kfac.shape for kfac in param.kfac])\n",
    "    print(\".kflr (shapes):          \", [kflr.shape for kflr in param.kflr])\n",
    "    print(\".kfra (shapes):          \", [kfra.shape for kfra in param.kfra])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonal Hessian and per-sample diagonal Hessian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(DiagHessian(), BatchDiagHessian()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".diag_h.shape:           \", param.diag_h.shape)\n",
    "    print(\".diag_h_batch.shape:     \", param.diag_h_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix square root of the generalized Gauss-Newton or its Monte-Carlo approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(SqrtGGNExact(), SqrtGGNMC(mc_samples=1)):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".sqrt_ggn_exact.shape:   \", param.sqrt_ggn_exact.shape)\n",
    "    print(\".sqrt_ggn_mc.shape:      \", param.sqrt_ggn_mc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block-diagonal curvature products\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curvature-matrix product (``MP``) extensions provide functions\n",
    "that multiply with the block diagonal of different curvature matrices, such as\n",
    "\n",
    "- the Hessian (:code:`HMP`)\n",
    "- the generalized Gauss-Newton (:code:`GGNMP`)\n",
    "- the positive-curvature Hessian (:code:`PCHMP`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "\n",
    "with backpack(\n",
    "    HMP(),\n",
    "    GGNMP(),\n",
    "    PCHMP(savefield=\"pchmp_clip\", modify=\"clip\"),\n",
    "    PCHMP(savefield=\"pchmp_abs\", modify=\"abs\"),\n",
    "):\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply a random vector with curvature blocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 1\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    vec = rand(V, *param.shape)\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\"vec.shape:               \", vec.shape)\n",
    "    print(\".hmp(vec).shape:         \", param.hmp(vec).shape)\n",
    "    print(\".ggnmp(vec).shape:       \", param.ggnmp(vec).shape)\n",
    "    print(\".pchmp_clip(vec).shape:  \", param.pchmp_clip(vec).shape)\n",
    "    print(\".pchmp_abs(vec).shape:   \", param.pchmp_abs(vec).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply a collection of three vectors (a matrix) with curvature blocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 3\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    vec = rand(V, *param.shape)\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\"vec.shape:               \", vec.shape)\n",
    "    print(\".hmp(vec).shape:         \", param.hmp(vec).shape)\n",
    "    print(\".ggnmp(vec).shape:       \", param.ggnmp(vec).shape)\n",
    "    print(\".pchmp_clip(vec).shape:  \", param.pchmp_clip(vec).shape)\n",
    "    print(\".pchmp_abs(vec).shape:   \", param.pchmp_abs(vec).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
