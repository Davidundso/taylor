{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Example using all extensions\n",
    "\n",
    "Basic example showing how compute the gradient,\n",
    "and and other quantities with BackPACK,\n",
    "on a linear model for MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/davidsuckrow/Documents/Developing/bachelor_thesis/experiments/exp_03_searching_values', '/opt/anaconda3/envs/taylor/lib/python38.zip', '/opt/anaconda3/envs/taylor/lib/python3.8', '/opt/anaconda3/envs/taylor/lib/python3.8/lib-dynload', '', '/opt/anaconda3/envs/taylor/lib/python3.8/site-packages', '/opt/anaconda3/envs/taylor/lib/python3.8/site-packages/setuptools/_vendor']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading some dummy data and extending the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import rand\n",
    "from torch.nn import CrossEntropyLoss, Flatten, Linear, Sequential\n",
    "\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import (\n",
    "    GGNMP,\n",
    "    HMP,\n",
    "    KFAC,\n",
    "    KFLR,\n",
    "    KFRA,\n",
    "    PCHMP,\n",
    "    BatchDiagGGNExact,\n",
    "    BatchDiagGGNMC,\n",
    "    BatchDiagHessian,\n",
    "    BatchGrad,\n",
    "    BatchL2Grad,\n",
    "    DiagGGNExact,\n",
    "    DiagGGNMC,\n",
    "    DiagHessian,\n",
    "    SqrtGGNExact,\n",
    "    SqrtGGNMC,\n",
    "    SumGradSquared,\n",
    "    Variance,\n",
    ")\n",
    "from backpack.utils.examples import load_one_batch_mnist\n",
    "\n",
    "X, y = load_one_batch_mnist(batch_size=512)\n",
    "\n",
    "model = Sequential(Flatten(), Linear(784, 10))\n",
    "lossfunc = CrossEntropyLoss()\n",
    "\n",
    "model = extend(model)\n",
    "lossfunc = extend(lossfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First order extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".grad_batch.shape:        torch.Size([512, 10, 784])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".grad_batch.shape:        torch.Size([512, 10])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(BatchGrad()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".grad_batch.shape:       \", param.grad_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".variance.shape:          torch.Size([10, 784])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".variance.shape:          torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(Variance()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".variance.shape:         \", param.variance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second moment/sum of gradients squared\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".sum_grad_squared.shape:  torch.Size([10, 784])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".sum_grad_squared.shape:  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(SumGradSquared()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".sum_grad_squared.shape: \", param.sum_grad_squared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 norm of individual gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".batch_l2.shape:          torch.Size([512])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".batch_l2.shape:          torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(BatchL2Grad()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".batch_l2.shape:         \", param.batch_l2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to ask for multiple quantities at once\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".grad_batch.shape:        torch.Size([512, 10, 784])\n",
      ".variance.shape:          torch.Size([10, 784])\n",
      ".sum_grad_squared.shape:  torch.Size([10, 784])\n",
      ".batch_l2.shape:          torch.Size([512])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".grad_batch.shape:        torch.Size([512, 10])\n",
      ".variance.shape:          torch.Size([10])\n",
      ".sum_grad_squared.shape:  torch.Size([10])\n",
      ".batch_l2.shape:          torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(BatchGrad(), Variance(), SumGradSquared(), BatchL2Grad()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".grad_batch.shape:       \", param.grad_batch.shape)\n",
    "    print(\".variance.shape:         \", param.variance.shape)\n",
    "    print(\".sum_grad_squared.shape: \", param.sum_grad_squared.shape)\n",
    "    print(\".batch_l2.shape:         \", param.batch_l2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonal of the generalized Gauss-Newton and its Monte-Carlo approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".diag_ggn_mc.shape:       torch.Size([10, 784])\n",
      ".diag_ggn_exact.shape:    torch.Size([10, 784])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".diag_ggn_mc.shape:       torch.Size([10])\n",
      ".diag_ggn_exact.shape:    torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(DiagGGNExact(), DiagGGNMC(mc_samples=1)):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".diag_ggn_mc.shape:      \", param.diag_ggn_mc.shape)\n",
    "    print(\".diag_ggn_exact.shape:   \", param.diag_ggn_exact.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per-sample diagonal of the generalized Gauss-Newton and its Monte-Carlo approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".diag_ggn_mc_batch.shape:       torch.Size([512, 10, 784])\n",
      ".diag_ggn_exact_batch.shape:    torch.Size([512, 10, 784])\n",
      "1.bias\n",
      ".diag_ggn_mc_batch.shape:       torch.Size([512, 10])\n",
      ".diag_ggn_exact_batch.shape:    torch.Size([512, 10])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(BatchDiagGGNExact(), BatchDiagGGNMC(mc_samples=1)):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".diag_ggn_mc_batch.shape:      \", param.diag_ggn_mc_batch.shape)\n",
    "    print(\".diag_ggn_exact_batch.shape:   \", param.diag_ggn_exact_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFAC, KFRA and KFLR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".kfac (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]\n",
      ".kflr (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]\n",
      ".kfra (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".kfac (shapes):           [torch.Size([10, 10])]\n",
      ".kflr (shapes):           [torch.Size([10, 10])]\n",
      ".kfra (shapes):           [torch.Size([10, 10])]\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(KFAC(mc_samples=1), KFLR(), KFRA()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".kfac (shapes):          \", [kfac.shape for kfac in param.kfac])\n",
    "    print(\".kflr (shapes):          \", [kflr.shape for kflr in param.kflr])\n",
    "    print(\".kfra (shapes):          \", [kfra.shape for kfra in param.kfra])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonal Hessian and per-sample diagonal Hessian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".diag_h.shape:            torch.Size([10, 784])\n",
      ".diag_h_batch.shape:      torch.Size([512, 10, 784])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".diag_h.shape:            torch.Size([10])\n",
      ".diag_h_batch.shape:      torch.Size([512, 10])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(DiagHessian(), BatchDiagHessian()):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".diag_h.shape:           \", param.diag_h.shape)\n",
    "    print(\".diag_h_batch.shape:     \", param.diag_h_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix square root of the generalized Gauss-Newton or its Monte-Carlo approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      ".sqrt_ggn_exact.shape:    torch.Size([10, 512, 10, 784])\n",
      ".sqrt_ggn_mc.shape:       torch.Size([1, 512, 10, 784])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      ".sqrt_ggn_exact.shape:    torch.Size([10, 512, 10])\n",
      ".sqrt_ggn_mc.shape:       torch.Size([1, 512, 10])\n"
     ]
    }
   ],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "with backpack(SqrtGGNExact(), SqrtGGNMC(mc_samples=1)):\n",
    "    loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\".sqrt_ggn_exact.shape:   \", param.sqrt_ggn_exact.shape)\n",
    "    print(\".sqrt_ggn_mc.shape:      \", param.sqrt_ggn_mc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block-diagonal curvature products\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curvature-matrix product (``MP``) extensions provide functions\n",
    "that multiply with the block diagonal of different curvature matrices, such as\n",
    "\n",
    "- the Hessian (:code:`HMP`)\n",
    "- the generalized Gauss-Newton (:code:`GGNMP`)\n",
    "- the positive-curvature Hessian (:code:`PCHMP`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossfunc(model(X), y)\n",
    "\n",
    "with backpack(\n",
    "    HMP(),\n",
    "    GGNMP(),\n",
    "    PCHMP(savefield=\"pchmp_clip\", modify=\"clip\"),\n",
    "    PCHMP(savefield=\"pchmp_abs\", modify=\"abs\"),\n",
    "):\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply a random vector with curvature blocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      "vec.shape:                torch.Size([1, 10, 784])\n",
      ".hmp(vec).shape:          torch.Size([1, 10, 784])\n",
      ".ggnmp(vec).shape:        torch.Size([1, 10, 784])\n",
      ".pchmp_clip(vec).shape:   torch.Size([1, 10, 784])\n",
      ".pchmp_abs(vec).shape:    torch.Size([1, 10, 784])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      "vec.shape:                torch.Size([1, 10])\n",
      ".hmp(vec).shape:          torch.Size([1, 10])\n",
      ".ggnmp(vec).shape:        torch.Size([1, 10])\n",
      ".pchmp_clip(vec).shape:   torch.Size([1, 10])\n",
      ".pchmp_abs(vec).shape:    torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "V = 1\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    vec = rand(V, *param.shape)\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\"vec.shape:               \", vec.shape)\n",
    "    print(\".hmp(vec).shape:         \", param.hmp(vec).shape)\n",
    "    print(\".ggnmp(vec).shape:       \", param.ggnmp(vec).shape)\n",
    "    print(\".pchmp_clip(vec).shape:  \", param.pchmp_clip(vec).shape)\n",
    "    print(\".pchmp_abs(vec).shape:   \", param.pchmp_abs(vec).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply a collection of three vectors (a matrix) with curvature blocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      ".grad.shape:              torch.Size([10, 784])\n",
      "vec.shape:                torch.Size([3, 10, 784])\n",
      ".hmp(vec).shape:          torch.Size([3, 10, 784])\n",
      ".ggnmp(vec).shape:        torch.Size([3, 10, 784])\n",
      ".pchmp_clip(vec).shape:   torch.Size([3, 10, 784])\n",
      ".pchmp_abs(vec).shape:    torch.Size([3, 10, 784])\n",
      "1.bias\n",
      ".grad.shape:              torch.Size([10])\n",
      "vec.shape:                torch.Size([3, 10])\n",
      ".hmp(vec).shape:          torch.Size([3, 10])\n",
      ".ggnmp(vec).shape:        torch.Size([3, 10])\n",
      ".pchmp_clip(vec).shape:   torch.Size([3, 10])\n",
      ".pchmp_abs(vec).shape:    torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "V = 3\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    vec = rand(V, *param.shape)\n",
    "    print(name)\n",
    "    print(\".grad.shape:             \", param.grad.shape)\n",
    "    print(\"vec.shape:               \", vec.shape)\n",
    "    print(\".hmp(vec).shape:         \", param.hmp(vec).shape)\n",
    "    print(\".ggnmp(vec).shape:       \", param.ggnmp(vec).shape)\n",
    "    print(\".pchmp_clip(vec).shape:  \", param.pchmp_clip(vec).shape)\n",
    "    print(\".pchmp_abs(vec).shape:   \", param.pchmp_abs(vec).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (taylor)",
   "language": "python",
   "name": "taylor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
