{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c6cf0f-0370-4908-a248-856d07e47c60",
   "metadata": {},
   "source": [
    "### Attempt to compute H*d (Block diagonal Hessian H, direction d)  \n",
    "\n",
    " First define a simple model (that is supported by Backpack):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1525587-1d61-408e-a49c-b78364594175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import rand\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import (\n",
    "    GGNMP,\n",
    "    HMP,\n",
    "    KFAC,\n",
    "    KFLR,\n",
    "    KFRA,\n",
    "    PCHMP,\n",
    "    BatchDiagGGNExact,\n",
    "    BatchDiagGGNMC,\n",
    "    BatchDiagHessian,\n",
    "    BatchGrad,\n",
    "    BatchL2Grad,\n",
    "    DiagGGNExact,\n",
    "    DiagGGNMC,\n",
    "    DiagHessian,\n",
    "    SqrtGGNExact,\n",
    "    SqrtGGNMC,\n",
    "    SumGradSquared,\n",
    "    Variance,\n",
    ")\n",
    "from backpack.utils.examples import load_one_batch_mnist\n",
    "\n",
    "\n",
    "# Define a simple neural network with nn.ReLU instead of torch.relu\n",
    "# Define a simple neural network using Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),  # Use nn.ReLU module directly in Sequential\n",
    "    nn.Linear(5, 1)\n",
    ")\n",
    "\n",
    "# The following is only to create a somewhat learnable datasetw\n",
    "# Set dimensions\n",
    "num_samples = 1000\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "\n",
    "# Create input data X with random values\n",
    "X = torch.randn(num_samples, input_dim)\n",
    "\n",
    "# Define a random weight matrix and bias for generating learnable y\n",
    "true_weights = torch.randn(input_dim, output_dim)\n",
    "true_bias = torch.randn(output_dim)\n",
    "\n",
    "# Generate target y as a linear combination of X plus some noise\n",
    "y = X @ true_weights + true_bias + 0.1 * torch.randn(num_samples, output_dim)\n",
    "\n",
    "# Check dimensions\n",
    "print(X.shape)  # Should be [1000, 10]\n",
    "print(y.shape)  # Should be [1000, 1]\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# IMPORTANT: extend youre model with backpack, otherwise the parameters will not be extended\n",
    "# Extend the model and criterion with BackPACK\n",
    "model = extend(model, use_converter=True) # Extend the loss function\n",
    "criterion = extend(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13b4e7-bea8-46eb-9c09-be128f1d39ed",
   "metadata": {},
   "source": [
    "Next we want to do a training with only one minibatch and one iteration for test purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e341d015-4503-4647-b6a4-eb0f5d7d3ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 1\n",
      "Parameter Name: 0.weight\n",
      "Parameter Dimension: torch.Size([5, 10])\n",
      "*param.shape              5 10\n",
      "vec.shape:                torch.Size([1, 5, 10])\n",
      ".hmp(vec).shape:          torch.Size([1, 5, 10])\n",
      "Parameter Name: 0.bias\n",
      "Parameter Dimension: torch.Size([5])\n",
      "*param.shape              5\n",
      "vec.shape:                torch.Size([1, 5])\n",
      ".hmp(vec).shape:          torch.Size([1, 5])\n",
      "Parameter Name: 2.weight\n",
      "Parameter Dimension: torch.Size([1, 5])\n",
      "*param.shape              1 5\n",
      "vec.shape:                torch.Size([1, 1, 5])\n",
      ".hmp(vec).shape:          torch.Size([1, 1, 5])\n",
      "Parameter Name: 2.bias\n",
      "Parameter Dimension: torch.Size([1])\n",
      "*param.shape              1\n",
      "vec.shape:                torch.Size([1, 1])\n",
      ".hmp(vec).shape:          torch.Size([1, 1])\n",
      "-d^T g for torch.Size([5, 10]): 2895.561279296875\n",
      "param.hmp(d).shape: torch.Size([5, 10])\n",
      "d^T H d for torch.Size([5, 10]): 21.463958740234375\n",
      "-d^T g for torch.Size([5]): 226.03884887695312\n",
      "param.hmp(d).shape: torch.Size([5])\n",
      "d^T H d for torch.Size([5]): 1.0212969779968262\n",
      "-d^T g for torch.Size([1, 5]): 1381.099365234375\n",
      "param.hmp(d).shape: torch.Size([1, 5])\n",
      "d^T H d for torch.Size([1, 5]): 215.84521484375\n",
      "-d^T g for torch.Size([1]): 42.47346878051758\n",
      "param.hmp(d).shape: torch.Size([1])\n",
      "d^T H d for torch.Size([1]): 0.3477383255958557\n",
      "Total d^T H d across all parameters: 238.67820888757706\n",
      "Total -d^T g across all parameters: 4545.172962188721\n",
      "a star:  19.04309984217776\n",
      "Loss :   tensor(1.5874, grad_fn=<BackwardHookFunctionBackward>)\n",
      "Epoch Number: 51\n",
      "Parameter Name: 0.weight\n",
      "Parameter Dimension: torch.Size([5, 10])\n",
      "*param.shape              5 10\n",
      "vec.shape:                torch.Size([1, 5, 10])\n",
      ".hmp(vec).shape:          torch.Size([1, 5, 10])\n",
      "Parameter Name: 0.bias\n",
      "Parameter Dimension: torch.Size([5])\n",
      "*param.shape              5\n",
      "vec.shape:                torch.Size([1, 5])\n",
      ".hmp(vec).shape:          torch.Size([1, 5])\n",
      "Parameter Name: 2.weight\n",
      "Parameter Dimension: torch.Size([1, 5])\n",
      "*param.shape              1 5\n",
      "vec.shape:                torch.Size([1, 1, 5])\n",
      ".hmp(vec).shape:          torch.Size([1, 1, 5])\n",
      "Parameter Name: 2.bias\n",
      "Parameter Dimension: torch.Size([1])\n",
      "*param.shape              1\n",
      "vec.shape:                torch.Size([1, 1])\n",
      ".hmp(vec).shape:          torch.Size([1, 1])\n",
      "-d^T g for torch.Size([5, 10]): 2813.931396484375\n",
      "param.hmp(d).shape: torch.Size([5, 10])\n",
      "d^T H d for torch.Size([5, 10]): 18.351123809814453\n",
      "-d^T g for torch.Size([5]): 306.96746826171875\n",
      "param.hmp(d).shape: torch.Size([5])\n",
      "d^T H d for torch.Size([5]): 1.10885488986969\n",
      "-d^T g for torch.Size([1, 5]): 2740.06201171875\n",
      "param.hmp(d).shape: torch.Size([1, 5])\n",
      "d^T H d for torch.Size([1, 5]): 444.442626953125\n",
      "-d^T g for torch.Size([1]): 14.691835403442383\n",
      "param.hmp(d).shape: torch.Size([1])\n",
      "d^T H d for torch.Size([1]): 0.11746750771999359\n",
      "Total d^T H d across all parameters: 464.02007316052914\n",
      "Total -d^T g across all parameters: 5875.652711868286\n",
      "a star:  12.662496843555648\n",
      "Loss :   tensor(0.9485, grad_fn=<BackwardHookFunctionBackward>)\n",
      "Epoch Number: 101\n",
      "Parameter Name: 0.weight\n",
      "Parameter Dimension: torch.Size([5, 10])\n",
      "*param.shape              5 10\n",
      "vec.shape:                torch.Size([1, 5, 10])\n",
      ".hmp(vec).shape:          torch.Size([1, 5, 10])\n",
      "Parameter Name: 0.bias\n",
      "Parameter Dimension: torch.Size([5])\n",
      "*param.shape              5\n",
      "vec.shape:                torch.Size([1, 5])\n",
      ".hmp(vec).shape:          torch.Size([1, 5])\n",
      "Parameter Name: 2.weight\n",
      "Parameter Dimension: torch.Size([1, 5])\n",
      "*param.shape              1 5\n",
      "vec.shape:                torch.Size([1, 1, 5])\n",
      ".hmp(vec).shape:          torch.Size([1, 1, 5])\n",
      "Parameter Name: 2.bias\n",
      "Parameter Dimension: torch.Size([1])\n",
      "*param.shape              1\n",
      "vec.shape:                torch.Size([1, 1])\n",
      ".hmp(vec).shape:          torch.Size([1, 1])\n",
      "-d^T g for torch.Size([5, 10]): 2917.855712890625\n",
      "param.hmp(d).shape: torch.Size([5, 10])\n",
      "d^T H d for torch.Size([5, 10]): 18.67940330505371\n",
      "-d^T g for torch.Size([5]): 309.9530029296875\n",
      "param.hmp(d).shape: torch.Size([5])\n",
      "d^T H d for torch.Size([5]): 0.7235028743743896\n",
      "-d^T g for torch.Size([1, 5]): 2609.068359375\n",
      "param.hmp(d).shape: torch.Size([1, 5])\n",
      "d^T H d for torch.Size([1, 5]): 548.2241821289062\n",
      "-d^T g for torch.Size([1]): 13.470452308654785\n",
      "param.hmp(d).shape: torch.Size([1])\n",
      "d^T H d for torch.Size([1]): 0.09848686307668686\n",
      "Total d^T H d across all parameters: 567.725575171411\n",
      "Total -d^T g across all parameters: 5850.347527503967\n",
      "a star:  10.304886345193356\n",
      "Loss :   tensor(0.9510, grad_fn=<BackwardHookFunctionBackward>)\n",
      "Epoch Number: 151\n",
      "Parameter Name: 0.weight\n",
      "Parameter Dimension: torch.Size([5, 10])\n",
      "*param.shape              5 10\n",
      "vec.shape:                torch.Size([1, 5, 10])\n",
      ".hmp(vec).shape:          torch.Size([1, 5, 10])\n",
      "Parameter Name: 0.bias\n",
      "Parameter Dimension: torch.Size([5])\n",
      "*param.shape              5\n",
      "vec.shape:                torch.Size([1, 5])\n",
      ".hmp(vec).shape:          torch.Size([1, 5])\n",
      "Parameter Name: 2.weight\n",
      "Parameter Dimension: torch.Size([1, 5])\n",
      "*param.shape              1 5\n",
      "vec.shape:                torch.Size([1, 1, 5])\n",
      ".hmp(vec).shape:          torch.Size([1, 1, 5])\n",
      "Parameter Name: 2.bias\n",
      "Parameter Dimension: torch.Size([1])\n",
      "*param.shape              1\n",
      "vec.shape:                torch.Size([1, 1])\n",
      ".hmp(vec).shape:          torch.Size([1, 1])\n",
      "-d^T g for torch.Size([5, 10]): 3144.42919921875\n",
      "param.hmp(d).shape: torch.Size([5, 10])\n",
      "d^T H d for torch.Size([5, 10]): 28.79451560974121\n",
      "-d^T g for torch.Size([5]): 297.1800537109375\n",
      "param.hmp(d).shape: torch.Size([5])\n",
      "d^T H d for torch.Size([5]): 1.2233831882476807\n",
      "-d^T g for torch.Size([1, 5]): 860.341552734375\n",
      "param.hmp(d).shape: torch.Size([1, 5])\n",
      "d^T H d for torch.Size([1, 5]): 226.92288208007812\n",
      "-d^T g for torch.Size([1]): 23.914709091186523\n",
      "param.hmp(d).shape: torch.Size([1])\n",
      "d^T H d for torch.Size([1]): 0.17359958589076996\n",
      "Total d^T H d across all parameters: 257.1143804639578\n",
      "Total -d^T g across all parameters: 4325.865514755249\n",
      "a star:  16.82467354327309\n",
      "Loss :   tensor(1.7850, grad_fn=<BackwardHookFunctionBackward>)\n",
      "Training complete!\n",
      "Maximum a_star value: 62.94567108154297\n",
      "Minimum a_star value: 10.26362419128418\n",
      "Standard deviation of a_star values: 14.95748233795166\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters for testing\n",
    "batch_size = 32\n",
    "n_epochs = 200  # Only one epoch for testing\n",
    "epoch_count = 0 # for print statements\n",
    "# List to store all a_star values\n",
    "a_star_values = []\n",
    "\n",
    "# Select one minibatch from the dataset\n",
    "X_batch = X[:batch_size]\n",
    "y_batch = y[:batch_size]\n",
    "\n",
    "# Training loop with only one minibatch\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    print_bool = True if epoch_count % 50 == 0 else False\n",
    "    # Forward pass\n",
    "    y_pred = model(X_batch)\n",
    "    loss = criterion(y_pred, y_batch)\n",
    "\n",
    "    with backpack(\n",
    "        HMP(),      # possible problem with comma, check if run fails\n",
    "        GGNMP(),\n",
    "    ):\n",
    "        loss.backward()\n",
    "\n",
    "    V = 1\n",
    "    if print_bool:\n",
    "        print(\"Epoch Number:\", epoch_count + 1)\n",
    "        for name, param in model.named_parameters():\n",
    "            # Access parameter name\n",
    "            print(\"Parameter Name:\", name)\n",
    "            print(\"Parameter Dimension:\", param.shape)\n",
    "            vec = rand(V, *param.shape)\n",
    "            print(\"*param.shape             \", *param.shape)\n",
    "            print(\"vec.shape:               \", vec.shape)\n",
    "            print(\".hmp(vec).shape:         \", param.hmp(vec).shape)\n",
    "\n",
    "    g_list = []  # List to store flattened gradients\n",
    "    d_list = []  # List to store flattened directions d_t\n",
    "    hessian_d_products = []  # To store d^T H d for each parameter\n",
    "    d_g_products = []  # To store -d^T g for each parameter\n",
    "\n",
    "    # Loop over parameters to compute and apply hmp in chunks\n",
    "    for param in model.parameters():  # parameters in this structure: a weight matrix of a layer or a bias vector of a layer\n",
    "        if param.grad is not None:\n",
    "            # Flatten and store the gradient for this parameter\n",
    "            g_list.append(param.grad.view(-1))  # make it a 1D tensor [1 0 1 1 ...]\n",
    "\n",
    "            # Initialize optimizer state if it doesn't exist: with zeros\n",
    "            # optimizer state = attributes stored by optimizer\n",
    "        if param not in optimizer.state:\n",
    "            optimizer.state[param]['exp_avg'] = torch.zeros_like(param.data)\n",
    "            optimizer.state[param]['exp_avg_sq'] = torch.zeros_like(param.data)\n",
    "            optimizer.state[param]['step'] = torch.tensor(0)  # Initialize step as tensor\n",
    "\n",
    "        # Ensure 'step' is a tensor\n",
    "        # step = number of updates applied to the parameter (stored by adam\n",
    "        if isinstance(optimizer.state[param]['step'], int):\n",
    "            optimizer.state[param]['step'] = torch.tensor(optimizer.state[param]['step'])\n",
    "\n",
    "        # Get Adam's internal state (first and second moments)\n",
    "        m_t = optimizer.state[param]['exp_avg']        # First moment (moving average of gradients)\n",
    "        v_t = optimizer.state[param]['exp_avg_sq']     # Second moment (moving average of squared gradients)\n",
    "\n",
    "        # Bias correction for moments\n",
    "        beta1, beta2 = optimizer.defaults['betas']\n",
    "        # optimizer.state[param]['step'] += 1  # Increment step as tensor # REMOVED: possible source of miscalculation\n",
    "        t = optimizer.state[param]['step'].item()  # Convert step tensor to int\n",
    "\n",
    "        m_t_hat = m_t / (1 - beta1**t)  # Bias-corrected first moment\n",
    "        v_t_hat = v_t / (1 - beta2**t)  # Bias-corrected second moment\n",
    "\n",
    "        # Compute Adam's update direction d_t\n",
    "        d_t = -m_t_hat / (torch.sqrt(v_t_hat) + optimizer.defaults['eps'])\n",
    "\n",
    "        # Flatten the direction d_t and append it to the list\n",
    "        d_list.append(d_t.view(-1))\n",
    "\n",
    "        # Compute -d^T g for this parameter\n",
    "        d_flat = d_t.view(-1)  # Flatten d\n",
    "        g_flat = param.grad.view(-1)  # Flatten the gradient g\n",
    "        d_g_product = -torch.dot(d_flat, g_flat)  # Compute -d^T g\n",
    "        d_g_products.append(d_g_product.item())  # Store the result as a scalar\n",
    "        \n",
    "        if print_bool:\n",
    "            print(f\"-d^T g for {param.shape}: {d_g_product}\")\n",
    "\n",
    "        # Reshape d_t to match the parameter's shape\n",
    "        vec = d_t.view(*param.shape)\n",
    "\n",
    "        # Perform the param.hmp(d) operation to compute H * d\n",
    "        hmp_output = param.hmp(vec)\n",
    "\n",
    "        # Compute d^T * H * d for this parameter\n",
    "        d_hmp_d = torch.dot(vec.view(-1), hmp_output.view(-1))  # Equivalent to d^T H d\n",
    "        hessian_d_products.append(d_hmp_d.item())  # Store the result as a scalar\n",
    "\n",
    "        if print_bool:\n",
    "            print(f\"param.hmp(d).shape: {hmp_output.shape}\")\n",
    "            print(f\"d^T H d for {param.shape}: {d_hmp_d}\")\n",
    "\n",
    "    # Now hessian_d_products contains d^T H d for each parameter.\n",
    "    total_d_H_d = sum(hessian_d_products)\n",
    "    if print_bool:\n",
    "        print(f\"Total d^T H d across all parameters: {total_d_H_d}\")\n",
    "\n",
    "    # Now d_g_products contains -d^T g for each parameter.\n",
    "    total_d_g = sum(d_g_products)\n",
    "    if print_bool:\n",
    "        print(f\"Total -d^T g across all parameters: {total_d_g}\")\n",
    "\n",
    "    # avoid division by zero\n",
    "    epsilon = 1e-8  # Small value to prevent division by zero\n",
    "    a_star = total_d_g / (total_d_H_d + epsilon)\n",
    "    # Store the a_star value\n",
    "    a_star_values.append(a_star)\n",
    "    \n",
    "    if print_bool:\n",
    "        print(\"a star: \", a_star)\n",
    "        print(\"Loss :  \", loss)\n",
    "    \n",
    "\n",
    "    # Optimizer step (updates the parameters)\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_count = epoch_count + 1\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# After the loop finishes\n",
    "a_star_tensor = torch.tensor(a_star_values)  # Convert list to a PyTorch tensor\n",
    "\n",
    "# Calculate statistics\n",
    "max_a_star = torch.max(a_star_tensor)\n",
    "min_a_star = torch.min(a_star_tensor)\n",
    "std_a_star = torch.std(a_star_tensor)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum a_star value: {max_a_star.item()}\")\n",
    "print(f\"Minimum a_star value: {min_a_star.item()}\")\n",
    "print(f\"Standard deviation of a_star values: {std_a_star.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (taylor)",
   "language": "python",
   "name": "taylor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
