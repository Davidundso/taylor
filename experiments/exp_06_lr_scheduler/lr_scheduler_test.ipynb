{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic learning-rate scheduler\n",
    "\n",
    "This notebook will attempt to write functions with approximately the following structure:\n",
    "\n",
    "```python\n",
    "class AdaptiveLROptimizer:\n",
    "\n",
    "    def __init__(self, model, loss_function, ...):\n",
    "        # TODO\n",
    "\n",
    "    def compute_alpha_star(self, direction, data, ...):\n",
    "        \"\"\"Compute the optimal alpha_* given the direction `direction` on the data `data`.\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def apply_step(self, direction, data, ...)\n",
    "        \"\"\"Compute the optimal alpha_* for the direction `direction` on the data `data`\n",
    "        and update the parameters of the model.\"\"\"\n",
    "        # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "# from torch.nn.utils import vector_to_parameters  # this is NOT inverse of params_to_vector\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from warnings import warn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from tueplots import axes, bundles, fonts, fontsizes, figsizes  # for consistent plotting\n",
    "\n",
    "from curvlinops import GGNLinearOperator, HessianLinearOperator\n",
    "from curvlinops.examples.functorch import functorch_ggn, functorch_hessian\n",
    "from curvlinops.examples.utils import report_nonclose\n",
    "from source.plotting import plot_data, get_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom class: AdaptiveLRoptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaptiveLROptimizer :\n",
    "    def __init__(self, model, loss_function, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def compute_alpha_star(self, data, target):\n",
    "        \"\"\"\n",
    "        Given data and target, computes the update direction d_unnormalized and the \"optimal\" step size alpha_star\n",
    "        \"\"\"\n",
    "        # Step 1: Save the current parameters (theta_0)\n",
    "        theta_0 = parameters_to_vector(self.model.parameters())  # brauche ich hier clone? Lukas fragen.\n",
    "\n",
    "        # Step 2: Zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Step 3: Forward pass to compute loss\n",
    "        output = self.model(data)\n",
    "        loss = self.loss_function(output, target)\n",
    "\n",
    "        # Step 4: Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Prepare data for GGN (Generalized Gauss-Newton)\n",
    "        Data = [(data, target)]  # Data as expected by GGNLinearOperator\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]  # Filter for trainable parameters\n",
    "\n",
    "        # Assuming GGNLinearOperator is already imported and available\n",
    "        GGN = GGNLinearOperator(self.model, self.loss_function, params, Data)  # Instantiate GGN operator\n",
    "        \n",
    "        # Step 6: Extract gradients and convert to a vector, move to 5\n",
    "        gradients = parameters_to_vector(param.grad for param in self.model.parameters() if param.grad is not None)\n",
    "\n",
    "        # Step 7: Perform optimizer step (this will change the parameters temporarily)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Step 8: Compute the direction of adjustment (d_unnormalized)\n",
    "        d_unnormalized = parameters_to_vector(self.model.parameters()) - theta_0\n",
    "\n",
    "        ## Step 9: Revert the model parameters back to the original state (theta_0)\n",
    "        #for param, original_param in zip(self.model.parameters(), theta_0.split([param.numel() for param in self.model.parameters()])):\n",
    "        #    param.data.copy_(original_param.data)\n",
    "        vector_to_parameters(theta_0, self.model.parameters())\n",
    "\n",
    "        GGNd = GGN @ d_unnormalized.detach().numpy()  # Multiply GGN * d, outputs np array\n",
    "\n",
    "        # Step 10: Compute alpha_* based on the direction (this is where you would implement your custom logic)\n",
    "        GGNd_tensor = torch.tensor(GGNd) # from_numpy()\n",
    "        \n",
    "        dGGNd = torch.dot(GGNd_tensor, d_unnormalized)\n",
    "        \n",
    "        dg = - torch.dot(gradients, d_unnormalized)  # numerator: - d^T*g\n",
    "        \n",
    "        alpha_star = dg / dGGNd\n",
    "\n",
    "        return alpha_star, d_unnormalized\n",
    "    \n",
    "\n",
    "    def apply_step(self, alpha_star, direction):\n",
    "        \"\"\"\n",
    "        Apply a step in the direction `direction` with step size `alpha_star`.\n",
    "        This updates the model parameters by taking a step along the direction of adjustment.\n",
    "        \"\"\"\n",
    "        # Step 1: Scale the direction by alpha_star\n",
    "        step_direction = alpha_star * direction\n",
    "\n",
    "        # Step 2: Update the parameters in the direction of `step_direction`\n",
    "        with torch.no_grad():  # Ensure no gradients are tracked\n",
    "            for param, step in zip(self.model.parameters(), step_direction.split([param.numel() for param in self.model.parameters()])):\n",
    "                param.add_(step.view_as(param))  # Apply the update to each parameter\n",
    "\n",
    "        # No need to return anything, as the model parameters are updated in place\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Does the class work? Does the update do the expected?\n",
    "\n",
    "Evaluated through a simple model and checking the values of direction d, alpha star and the consecutive param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: tensor(18.2899, grad_fn=<DivBackward0>)\n",
      "direction: tensor([ 0.0065,  0.0113, -0.0036, -0.0030, -0.0005, -0.0114,  0.0042, -0.0140,\n",
      "         0.0063,  0.0072, -0.0184], grad_fn=<SubBackward0>)\n",
      "update: d*alpha: tensor([ 0.1195,  0.2072, -0.0655, -0.0541, -0.0098, -0.2084,  0.0766, -0.2557,\n",
      "         0.1158,  0.1320, -0.3362], grad_fn=<MulBackward0>)\n",
      "Params before update:\n",
      "params as vector: tensor([-0.2978,  0.1916,  0.0600, -0.0790, -0.0622,  0.2537, -0.1593,  0.1321,\n",
      "        -0.0269,  0.1562, -0.1970], grad_fn=<CatBackward0>)\n",
      " expected updated params: tensor([-0.1783,  0.3988, -0.0056, -0.1331, -0.0720,  0.0453, -0.0827, -0.1236,\n",
      "         0.0889,  0.2882, -0.5332], grad_fn=<AddBackward0>)\n",
      "actual updated params: tensor([-0.1783,  0.3988, -0.0056, -0.1331, -0.0720,  0.0453, -0.0827, -0.1236,\n",
      "         0.0889,  0.2882, -0.5332], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Standard optimizer\n",
    "\n",
    "# Initialize your AdaptiveLROptimizer\n",
    "adaptive_optimizer = AdaptiveLROptimizer(model, loss_function, optimizer)\n",
    "\n",
    "# Example input data and target\n",
    "data = torch.randn(5, 10)  # Batch of 5 samples, each with 10 features\n",
    "target = torch.randn(5, 1)  # Corresponding target for each sample\n",
    "\n",
    "# Compute the optimal step size and direction\n",
    "alpha_star, direction = adaptive_optimizer.compute_alpha_star(data, target)\n",
    "\n",
    "print(\"alpha:\", alpha_star)\n",
    "print(\"direction:\", direction)\n",
    "print(\"update: d*alpha:\", alpha_star*direction)\n",
    "\n",
    "print(\"Params before update:\")\n",
    "\"\"\"\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter values: {param.data}\")\n",
    "    print(f\"Gradient: {param.grad}\")\n",
    "    print(\"=\" * 50)\n",
    "\"\"\"\n",
    "\n",
    "print(\"params as vector:\", parameters_to_vector(model.parameters()))\n",
    "\n",
    "print(\" expected updated params:\", parameters_to_vector(model.parameters()) + (alpha_star*direction))\n",
    "\n",
    "\n",
    "# Apply the update step using the computed alpha_star and direction\n",
    "adaptive_optimizer.apply_step(alpha_star, direction)\n",
    "\n",
    "print(\"actual updated params:\", parameters_to_vector(model.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: One dimensional optimal step\n",
    "\n",
    "- defining a linear model with one parameter theta = 0, one input x = x, target y = y\n",
    "- MSE loss and SGD \n",
    "- using AdaptiveLROptimizer class for the step\n",
    "\n",
    "$$\n",
    "MSE=\\frac{1}{N} \\sum_{i=1}^n\\left(y_i-\\hat{y}\\right)^2\n",
    "$$\n",
    "\n",
    "$\\hspace{7cm}$ For $N=1$ :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& MSE=(y-\\hat{y})^2 \\\\\n",
    "& =(x \\cdot \\theta-\\hat{y})^2 \\\\\n",
    "& \\frac{\\partial M S E}{\\partial \\theta}=2(x \\cdot \\theta-\\hat{y}) \\quad x=0 \\\\\n",
    "& \\Leftrightarrow \\quad 2 x(x \\theta-\\hat{y})=0 \\\\\n",
    "& \\Leftrightarrow \\quad 2 x^2 \\theta-2 x \\hat{y}=0 \\\\\n",
    "& \\Leftrightarrow \\quad \\theta=\\frac{\\hat{y}}{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$\\hspace{7cm} \\Rightarrow$ After the step $\\alpha^* \\cdot d  $ we $\\operatorname{expect} \\theta=\\frac{\\hat{y}}{x}$\n",
    "\n",
    "$\\hspace{7cm}$ Example: $x=2 ; \\hat{y}=5$\n",
    "$$\n",
    "\\Rightarrow \\theta=\\frac{5}{2}=2.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta before optimizer: 0.35794949531555176\n",
      " theta_0 = 0.35794949531555176\n",
      "input x = -1.2171871662139893\n",
      "target = -0.8010745644569397\n",
      "Computed alpha_star: 28.12376594543457\n",
      "Direction: 0.010673761367797852\n",
      "theta_1 = theta_0 + d * alpha_star = 0.6581358617809201\n",
      "Updated parameter value: 0.6581358909606934\n",
      "Target value theta1: tensor([0.6581])\n",
      "Test passed: Parameter stepped to:  0.6581358909606934  Analytically it should be: 0.6581358909606934\n",
      "Test passed: Model predicts -0.8010745644569397 The target is: -0.8010745644569397\n",
      "Test passed: Loss is : 0.0\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "def test_adaptive_lr_optimizer():\n",
    "    # Step 1: Define a simple 1D model\n",
    "    theta_0 = torch.randn(1)\n",
    "    theta_old = theta_0.clone()\n",
    "\n",
    "    class Simple1DModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Simple1DModel, self).__init__()\n",
    "            self.param = nn.Parameter(theta_0)  # Start at w = 0\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.param * x  # Linear model: y = w * x\n",
    "\n",
    "    # Step 2: Use built-in MSELoss, which is a quadratic loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "    # Step 3: Set up the model, optimizer, and custom adaptive optimizer\n",
    "    model = Simple1DModel()\n",
    "    sgd_optimizer = SGD(model.parameters(), lr=.012)  # Dummy optimizer\n",
    "    adaptive_optimizer = AdaptiveLROptimizer(model, loss_function, sgd_optimizer)\n",
    "\n",
    "    # Step 4: \n",
    "    target = torch.randn(1) \n",
    "    x = torch.randn(1)  # Input (doesn't matter in 1D)\n",
    "    print(\"theta before optimizer:\", model.param.item())\n",
    "    # Step 5: Use the AdaptiveLROptimizer to compute alpha_star and direction\n",
    "    alpha_star, direction = adaptive_optimizer.compute_alpha_star(x, target)\n",
    "    print(\" theta_0 =\", model.param.item())\n",
    "    print(\"input x =\", x.item())\n",
    "    print(\"target =\", target.item())\n",
    "    print(f\"Computed alpha_star: {alpha_star.item()}\")\n",
    "    print(f\"Direction: {direction.item()}\")\n",
    "    print(\"theta_1 = theta_0 + d * alpha_star =\", theta_old.item() + direction.item() * alpha_star.item())\n",
    "\n",
    "\n",
    "    # Step 6: Apply the step using AdaptiveLROptimizer\n",
    "    adaptive_optimizer.apply_step(alpha_star, direction)\n",
    "    \n",
    "    # Step 7: \n",
    "    theta_target = target / x  # Analytical solution for the optimal parameter value\n",
    "    print(f\"Updated parameter value: {model.param.item()}\")\n",
    "    print(f\"Target value theta1: {theta_target}\")\n",
    "\n",
    "    assert torch.isclose(model.param, theta_target , atol=1e-6), \"The parameter did not step to the right value!\"\n",
    "    print(\"Test passed: Parameter stepped to: \", model.param.item(), \" Analytically it should be:\", theta_target.item())\n",
    "\n",
    "    # Assert that the parameter is now close to the target minimum  \n",
    "    assert torch.isclose(model(x), target, atol=1e-6), \"The parameter model fit is not good enough (should be perfect)!\"\n",
    "    print(\"Test passed: Model predicts\", model(x).item(), \"The target is:\", target.item())\n",
    "\n",
    "    assert torch.isclose(loss_function(model(x), target), torch.tensor(0, dtype=torch.float32), atol=1e-6), \"The loss is not very close to 0\"\n",
    "    print(\"Test passed: Loss is :\", loss_function(model(x), target).item())\n",
    "\n",
    "# Run the test\n",
    "test_adaptive_lr_optimizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update_params method for AlgoPerf\n",
    "\n",
    "Implement the submission function update_params for testing the algorithm on AlgoPerf using the update by class AdaptiveLROptimizer\n",
    "\n",
    "\n",
    "```python\n",
    "def update_params(\n",
    "    workload: Workload,\n",
    "    current_param_container: ParameterContainer,\n",
    "    current_params_types: ParameterTypeTree,\n",
    "    model_state: ModelAuxiliaryState,\n",
    "    hyperparameters: Hyperparameters,\n",
    "    batch: Dict[str, Tensor],\n",
    "    loss_type: LossType,\n",
    "    optimizer_state: OptimizerState,\n",
    "    eval_results: List[Tuple[int, float]],\n",
    "    global_step: int,\n",
    "    rng: RandomState\n",
    ") -> (updated_optimizer_state, updated_variables, updated_model_state)\n",
    "```\n",
    "\n",
    "- `current_param_container` is the same kind of nested structure as used by `model_fn` which constitutes a nested collection of `float32` arrays, each endowed with information about what kind of parameter that array represents stored in a parallel structure of `current_params_types`.\n",
    "  - Parameter kind is one of {\"weights\", \"biases\", \"embeddings\", \"conv\", \"batch norm\"}.\n",
    "- `model_state` holds auxiliary state necessary for some models, such as the current batch norm statistics.\n",
    "- The loss function will be one of a small set of known possibilities and the update function is allowed to branch on the `loss_type` enum/name.\n",
    "- The `loss_fn` produces a loss per example and a summed loss (both only for one device), which both can be used.\n",
    "- Allowed to update state for the optimizer.\n",
    "- Uses the `model_fn` of the `workload` in order to decouple the loss from the model so that model outputs (forward passes) can be reused (by storing them in the optimizer state).\n",
    "- The submission can access the target evaluation metric via the `workload` variable.\n",
    "- **A call to this function will be considered a step**\n",
    "  - The time between a call to this function and the next call to this function will be considered the per-step time.\n",
    "- Cannot modify the given hyperparameters in a workload-conditional way (please see the [Valid submission](#valid-submissions) section). This rule is intended to prohibit circumventing the tuning rules by looking up a pre-tuned optimal set of hyperparameters for each workload. It is not intended to prohibit line searches and other similar techniques.\n",
    "- The fixed `init_model_fn` can optionally be called during training, for example, to reinitialize the model after a failed training effort.\n",
    "- Cannot replace the model parameters with pre-trained ones.\n",
    "- This API supports Polyak averaging and similar methods that implement moving averages of model parameters.\n",
    "- Batch norm should work here because the `model_fn` will return updated batch norm moving averages when it is told to with `update_batch_norm`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Workload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor  \u001b[38;5;66;03m# For the Tensor type\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# from your_module import Workload, ParameterContainer, ParameterTypeTree, ModelAuxiliaryState, Hyperparameters, LossType, OptimizerState  # Adjust according to your project structure\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# from your_optimizer_module import AdaptiveLROptimizer  # Adjust to import your custom optimizer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_params\u001b[39m(\n\u001b[0;32m---> 13\u001b[0m     workload: \u001b[43mWorkload\u001b[49m,\n\u001b[1;32m     14\u001b[0m     current_param_container: ParameterContainer,\n\u001b[1;32m     15\u001b[0m     current_params_types: ParameterTypeTree,\n\u001b[1;32m     16\u001b[0m     model_state: ModelAuxiliaryState,\n\u001b[1;32m     17\u001b[0m     hyperparameters: Hyperparameters,\n\u001b[1;32m     18\u001b[0m     batch: Dict[\u001b[38;5;28mstr\u001b[39m, Tensor],\n\u001b[1;32m     19\u001b[0m     loss_type: LossType,\n\u001b[1;32m     20\u001b[0m     optimizer_state: OptimizerState,\n\u001b[1;32m     21\u001b[0m     eval_results: List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]],\n\u001b[1;32m     22\u001b[0m     global_step: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     23\u001b[0m     rng): \u001b[38;5;66;03m# rng: RandomState\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Step 1: Initialize your model and loss function\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     model \u001b[38;5;241m=\u001b[39m current_param_container\u001b[38;5;241m.\u001b[39mmodel  \u001b[38;5;66;03m# Assuming model is part of your ParameterContainer\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     loss_function \u001b[38;5;241m=\u001b[39m loss_type  \u001b[38;5;66;03m# This should be defined based on your context\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Workload' is not defined"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# Need to fork AlgoPerf next to make this work. \n",
    "# \n",
    "\n",
    "\n",
    "from typing import Dict, List, Tuple  # For type hinting\n",
    "import torch  # For tensor operations and optimizers\n",
    "from torch import Tensor  # For the Tensor type\n",
    "# from your_module import Workload, ParameterContainer, ParameterTypeTree, ModelAuxiliaryState, Hyperparameters, LossType, OptimizerState  # Adjust according to your project structure\n",
    "# from your_optimizer_module import AdaptiveLROptimizer  # Adjust to import your custom optimizer\n",
    "\n",
    "def update_params(\n",
    "    workload: Workload,\n",
    "    current_param_container: ParameterContainer,\n",
    "    current_params_types: ParameterTypeTree,\n",
    "    model_state: ModelAuxiliaryState,\n",
    "    hyperparameters: Hyperparameters,\n",
    "    batch: Dict[str, Tensor],\n",
    "    loss_type: LossType,\n",
    "    optimizer_state: OptimizerState,\n",
    "    eval_results: List[Tuple[int, float]],\n",
    "    global_step: int,\n",
    "    rng): # rng: RandomState\n",
    "    \n",
    "    # Step 1: Initialize your model and loss function\n",
    "    model = current_param_container.model  # Assuming model is part of your ParameterContainer\n",
    "    loss_function = loss_type  # This should be defined based on your context\n",
    "\n",
    "    # Step 2: Create an instance of the AdaptiveLROptimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters.learning_rate)\n",
    "    adaptive_optimizer = AdaptiveLROptimizer(model, loss_function, optimizer)\n",
    "\n",
    "    # Step 3: Get the data and target from the batch\n",
    "    data = batch['data']\n",
    "    target = batch['target']\n",
    "\n",
    "    # Step 4: Compute alpha_star and the direction d\n",
    "    alpha_star, direction = adaptive_optimizer.compute_alpha_star(data, target)\n",
    "\n",
    "    # Step 5: Apply the update step using the computed values\n",
    "    adaptive_optimizer.apply_step(alpha_star, direction)\n",
    "\n",
    "    # Step 6: Update optimizer state if needed\n",
    "    # (This may depend on how you manage the optimizer state in your framework)\n",
    "\n",
    "    # Step 7: Return updated states\n",
    "    updated_optimizer_state = optimizer_state  # Adjust as necessary based on your framework\n",
    "    updated_variables = current_param_container.variables  # Adjust if necessary\n",
    "    updated_model_state = model_state  # Update as needed if you modify the model state\n",
    "\n",
    "    return updated_optimizer_state, updated_variables, updated_model_state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taylor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
