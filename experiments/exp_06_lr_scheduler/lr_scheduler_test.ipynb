{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic learning-rate scheduler\n",
    "\n",
    "This notebook will attempt to write functions with approximately the following structure:\n",
    "\n",
    "```python\n",
    "class AdaptiveLROptimizer:\n",
    "\n",
    "    def __init__(self, model, loss_function, ...):\n",
    "        # TODO\n",
    "\n",
    "    def compute_alpha_star(self, direction, data, ...):\n",
    "        \"\"\"Compute the optimal alpha_* given the direction `direction` on the data `data`.\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def apply_step(self, direction, data, ...)\n",
    "        \"\"\"Compute the optimal alpha_* for the direction `direction` on the data `data`\n",
    "        and update the parameters of the model.\"\"\"\n",
    "        # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from torch.nn.utils import vector_to_parameters  # this is NOT inverse of params_to_vector\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from warnings import warn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tueplots import axes, bundles, fonts, fontsizes, figsizes  # for consistent plotting\n",
    "\n",
    "from curvlinops import GGNLinearOperator, HessianLinearOperator\n",
    "from curvlinops.examples.functorch import functorch_ggn, functorch_hessian\n",
    "from curvlinops.examples.utils import report_nonclose\n",
    "from source.plotting import plot_data, get_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom class: AdaptiveLROptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaptiveLROptimizer :\n",
    "    def __init__(self, model, loss_function, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def compute_alpha_star(self, data, target):\n",
    "        \"\"\"\n",
    "        Given data and target, computes the update direction d_unnormalized and the \"optimal\" step size alpha_star\n",
    "        \"\"\"\n",
    "        # Step 1: Save the current parameters (theta_0)\n",
    "        theta_0 = parameters_to_vector(self.model.parameters()).clone()  # brauche ich hier clone? Lukas fragen\n",
    "\n",
    "        # Step 2: Zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Step 3: Forward pass to compute loss\n",
    "        output = self.model(data)\n",
    "        loss = self.loss_function(output, target)\n",
    "\n",
    "        # Step 4: Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Prepare data for GGN (Generalized Gauss-Newton)\n",
    "        Data = [(data, target)]  # Data as expected by GGNLinearOperator\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]  # Filter for trainable parameters\n",
    "\n",
    "        # Assuming GGNLinearOperator is already imported and available\n",
    "        GGN = GGNLinearOperator(self.model, self.loss_function, params, Data)  # Instantiate GGN operator\n",
    "        \n",
    "        # Step 6: Extract gradients and convert to a vector\n",
    "        gradients = parameters_to_vector(param.grad for param in self.model.parameters() if param.grad is not None)\n",
    "\n",
    "        # Step 7: Perform optimizer step (this will change the parameters temporarily)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Step 8: Compute the direction of adjustment (d_unnormalized)\n",
    "        d_unnormalized = parameters_to_vector(self.model.parameters()) - theta_0\n",
    "\n",
    "        # Step 9: Revert the model parameters back to the original state (theta_0)\n",
    "        for param, original_param in zip(self.model.parameters(), theta_0.split([param.numel() for param in self.model.parameters()])):\n",
    "            param.data.copy_(original_param.data)\n",
    "\n",
    "        GGNd = GGN @ d_unnormalized.detach().numpy()  # Multiply GGN * d, outputs np array\n",
    "\n",
    "        # Step 10: Compute alpha_* based on the direction (this is where you would implement your custom logic)\n",
    "        GGNd_tensor = torch.tensor(GGNd)\n",
    "        \n",
    "        dGGNd = torch.dot(GGNd_tensor, d_unnormalized)\n",
    "        \n",
    "        dg = - torch.dot(gradients, d_unnormalized)  # numerator: - d^T*g\n",
    "        \n",
    "        alpha_star = dg / dGGNd\n",
    "\n",
    "        return alpha_star, d_unnormalized\n",
    "    \n",
    "\n",
    "    def apply_step(self, alpha_star, direction):\n",
    "        \"\"\"\n",
    "        Apply a step in the direction `direction` with step size `alpha_star`.\n",
    "        This updates the model parameters by taking a step along the direction of adjustment.\n",
    "        \"\"\"\n",
    "        # Step 1: Scale the direction by alpha_star\n",
    "        step_direction = alpha_star * direction\n",
    "\n",
    "        # Step 2: Update the parameters in the direction of `step_direction`\n",
    "        with torch.no_grad():  # Ensure no gradients are tracked\n",
    "            for param, step in zip(self.model.parameters(), step_direction.split([param.numel() for param in self.model.parameters()])):\n",
    "                param.add_(step.view_as(param))  # Apply the update to each parameter\n",
    "\n",
    "        # No need to return anything, as the model parameters are updated in place\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Does the class work? Does the update do the expected?\n",
    "\n",
    "Evaluated through a simple model and checking the values of direction d, alpha star and the consecutive param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: tensor(8.4657, grad_fn=<DivBackward0>)\n",
      "direction: tensor([-0.0021, -0.0033, -0.0163,  0.0179,  0.0216,  0.0192,  0.0155,  0.0288,\n",
      "        -0.0019,  0.0135, -0.0158], grad_fn=<SubBackward0>)\n",
      "update: d*alpha*: tensor([-0.0181, -0.0283, -0.1376,  0.1515,  0.1832,  0.1624,  0.1315,  0.2438,\n",
      "        -0.0165,  0.1142, -0.1339], grad_fn=<MulBackward0>)\n",
      "Params before update:\n",
      "params as vector: tensor([-0.0033, -0.2710, -0.2067, -0.1181,  0.1476, -0.1670, -0.3112, -0.3039,\n",
      "         0.0883,  0.0596,  0.1239], grad_fn=<CatBackward0>)\n",
      " expected updated params: tensor([-0.0214, -0.2993, -0.3444,  0.0334,  0.3308, -0.0045, -0.1798, -0.0601,\n",
      "         0.0718,  0.1738, -0.0100], grad_fn=<AddBackward0>)\n",
      "actual updated params: tensor([-0.0214, -0.2993, -0.3444,  0.0334,  0.3308, -0.0045, -0.1798, -0.0601,\n",
      "         0.0718,  0.1738, -0.0100], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Standard optimizer\n",
    "\n",
    "# Initialize your AdaptiveLROptimizer\n",
    "adaptive_optimizer = AdaptiveLROptimizer(model, loss_function, optimizer)\n",
    "\n",
    "# Example input data and target\n",
    "data = torch.randn(5, 10)  # Batch of 5 samples, each with 10 features\n",
    "target = torch.randn(5, 1)  # Corresponding target for each sample\n",
    "\n",
    "# Compute the optimal step size and direction\n",
    "alpha_star, direction = adaptive_optimizer.compute_alpha_star(data, target)\n",
    "\n",
    "print(\"alpha:\", alpha_star)\n",
    "print(\"direction:\", direction)\n",
    "print(\"update: d*alpha*:\", alpha_star*direction)\n",
    "\n",
    "print(\"Params before update:\")\n",
    "\"\"\"\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter values: {param.data}\")\n",
    "    print(f\"Gradient: {param.grad}\")\n",
    "    print(\"=\" * 50)\n",
    "\"\"\"\n",
    "\n",
    "print(\"params as vector:\", parameters_to_vector(model.parameters()))\n",
    "\n",
    "print(\" expected updated params:\", parameters_to_vector(model.parameters()) + (alpha_star*direction))\n",
    "\n",
    "\n",
    "# Apply the update step using the computed alpha_star and direction\n",
    "adaptive_optimizer.apply_step(alpha_star, direction)\n",
    "\n",
    "print(\"actual updated params:\", parameters_to_vector(model.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: One dimensional optimal step\n",
    "\n",
    "- defining a linear model with one parameter p = 0, one input x = 1, target y = 2\n",
    "- MSE loss and SGD \n",
    "- using AdaptiveLROptimizer class for the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed alpha_star: 416.6666564941406\n",
      "Direction: 0.004800000227987766\n",
      "Updated parameter value: 2.0\n",
      "Target value (w*): 2.0\n",
      "Test passed: Parameter successfully stepped to the minimum.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "def test_adaptive_lr_optimizer():\n",
    "    # Step 1: Define a simple 1D model\n",
    "    class Simple1DModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Simple1DModel, self).__init__()\n",
    "            self.param = nn.Parameter(torch.tensor([0.0]))  # Start at w = 0\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.param * x  # Linear model: y = w * x\n",
    "\n",
    "    # Step 2: Use built-in MSELoss, which is a quadratic loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "    # Step 3: Set up the model, optimizer, and custom adaptive optimizer\n",
    "    model = Simple1DModel()\n",
    "    sgd_optimizer = SGD(model.parameters(), lr=.0012)  # Dummy optimizer\n",
    "    adaptive_optimizer = AdaptiveLROptimizer(model, loss_function, sgd_optimizer)\n",
    "\n",
    "    # Step 4: Known minimum of the loss function is at w* = 2\n",
    "    target = torch.tensor([2.0])  # The parabola's minimum is at w = 2\n",
    "    x = torch.tensor([1.0])  # Input x = 1 (doesn't matter in 1D)\n",
    "\n",
    "    # Step 5: Use the AdaptiveLROptimizer to compute alpha_star and direction\n",
    "    alpha_star, direction = adaptive_optimizer.compute_alpha_star(x, target)\n",
    "    \n",
    "    print(f\"Computed alpha_star: {alpha_star.item()}\")\n",
    "    print(f\"Direction: {direction.item()}\")\n",
    "\n",
    "    # Step 6: Apply the step using AdaptiveLROptimizer\n",
    "    adaptive_optimizer.apply_step(alpha_star, direction)\n",
    "    \n",
    "    # Step 7: Check if the parameter stepped close to the minimum (w* = 2)\n",
    "    print(f\"Updated parameter value: {model.param.item()}\")\n",
    "    print(f\"Target value (w*): {target.item()}\")\n",
    "\n",
    "    # Assert that the parameter is now close to the target minimum (w* = 2)\n",
    "    assert torch.isclose(model.param, target, atol=1e-6), \"The parameter did not step to the minimum!\"\n",
    "    print(\"Test passed: Parameter successfully stepped to the minimum.\")\n",
    "\n",
    "# Run the test\n",
    "test_adaptive_lr_optimizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update_params method for AlgoPerf\n",
    "\n",
    "Implement the submission function update_params for testing the algorithm on AlgoPerf using the update by class AdaptiveLROptimizer\n",
    "\n",
    "\n",
    "```python\n",
    "def update_params(\n",
    "    workload: Workload,\n",
    "    current_param_container: ParameterContainer,\n",
    "    current_params_types: ParameterTypeTree,\n",
    "    model_state: ModelAuxiliaryState,\n",
    "    hyperparameters: Hyperparameters,\n",
    "    batch: Dict[str, Tensor],\n",
    "    loss_type: LossType,\n",
    "    optimizer_state: OptimizerState,\n",
    "    eval_results: List[Tuple[int, float]],\n",
    "    global_step: int,\n",
    "    rng: RandomState\n",
    ") -> (updated_optimizer_state, updated_variables, updated_model_state)\n",
    "```\n",
    "\n",
    "- `current_param_container` is the same kind of nested structure as used by `model_fn` which constitutes a nested collection of `float32` arrays, each endowed with information about what kind of parameter that array represents stored in a parallel structure of `current_params_types`.\n",
    "  - Parameter kind is one of {\"weights\", \"biases\", \"embeddings\", \"conv\", \"batch norm\"}.\n",
    "- `model_state` holds auxiliary state necessary for some models, such as the current batch norm statistics.\n",
    "- The loss function will be one of a small set of known possibilities and the update function is allowed to branch on the `loss_type` enum/name.\n",
    "- The `loss_fn` produces a loss per example and a summed loss (both only for one device), which both can be used.\n",
    "- Allowed to update state for the optimizer.\n",
    "- Uses the `model_fn` of the `workload` in order to decouple the loss from the model so that model outputs (forward passes) can be reused (by storing them in the optimizer state).\n",
    "- The submission can access the target evaluation metric via the `workload` variable.\n",
    "- **A call to this function will be considered a step**\n",
    "  - The time between a call to this function and the next call to this function will be considered the per-step time.\n",
    "- Cannot modify the given hyperparameters in a workload-conditional way (please see the [Valid submission](#valid-submissions) section). This rule is intended to prohibit circumventing the tuning rules by looking up a pre-tuned optimal set of hyperparameters for each workload. It is not intended to prohibit line searches and other similar techniques.\n",
    "- The fixed `init_model_fn` can optionally be called during training, for example, to reinitialize the model after a failed training effort.\n",
    "- Cannot replace the model parameters with pre-trained ones.\n",
    "- This API supports Polyak averaging and similar methods that implement moving averages of model parameters.\n",
    "- Batch norm should work here because the `model_fn` will return updated batch norm moving averages when it is told to with `update_batch_norm`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'your_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# For tensor operations and optimizers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor  \u001b[38;5;66;03m# For the Tensor type\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myour_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workload, ParameterContainer, ParameterTypeTree, ModelAuxiliaryState, Hyperparameters, LossType, OptimizerState  \u001b[38;5;66;03m# Adjust according to your project structure\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myour_optimizer_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaptiveLROptimizer  \u001b[38;5;66;03m# Adjust to import your custom optimizer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_params\u001b[39m(\n\u001b[1;32m      8\u001b[0m     workload: Workload,\n\u001b[1;32m      9\u001b[0m     current_param_container: ParameterContainer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Step 1: Initialize your model and loss function\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'your_module'"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# Need to fork AlgoPerf next to make this work. \n",
    "# \n",
    "\n",
    "\n",
    "from typing import Dict, List, Tuple  # For type hinting\n",
    "import torch  # For tensor operations and optimizers\n",
    "from torch import Tensor  # For the Tensor type\n",
    "# from your_module import Workload, ParameterContainer, ParameterTypeTree, ModelAuxiliaryState, Hyperparameters, LossType, OptimizerState  # Adjust according to your project structure\n",
    "# from your_optimizer_module import AdaptiveLROptimizer  # Adjust to import your custom optimizer\n",
    "\n",
    "def update_params(\n",
    "    workload: Workload,\n",
    "    current_param_container: ParameterContainer,\n",
    "    current_params_types: ParameterTypeTree,\n",
    "    model_state: ModelAuxiliaryState,\n",
    "    hyperparameters: Hyperparameters,\n",
    "    batch: Dict[str, Tensor],\n",
    "    loss_type: LossType,\n",
    "    optimizer_state: OptimizerState,\n",
    "    eval_results: List[Tuple[int, float]],\n",
    "    global_step: int,\n",
    "    rng): # rng: RandomState\n",
    "    \n",
    "    # Step 1: Initialize your model and loss function\n",
    "    model = current_param_container.model  # Assuming model is part of your ParameterContainer\n",
    "    loss_function = loss_type  # This should be defined based on your context\n",
    "\n",
    "    # Step 2: Create an instance of the AdaptiveLROptimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters.learning_rate)\n",
    "    adaptive_optimizer = AdaptiveLROptimizer(model, loss_function, optimizer)\n",
    "\n",
    "    # Step 3: Get the data and target from the batch\n",
    "    data = batch['data']\n",
    "    target = batch['target']\n",
    "\n",
    "    # Step 4: Compute alpha_star and the direction d\n",
    "    alpha_star, direction = adaptive_optimizer.compute_alpha_star(data, target)\n",
    "\n",
    "    # Step 5: Apply the update step using the computed values\n",
    "    adaptive_optimizer.apply_step(alpha_star, direction)\n",
    "\n",
    "    # Step 6: Update optimizer state if needed\n",
    "    # (This may depend on how you manage the optimizer state in your framework)\n",
    "\n",
    "    # Step 7: Return updated states\n",
    "    updated_optimizer_state = optimizer_state  # Adjust as necessary based on your framework\n",
    "    updated_variables = current_param_container.variables  # Adjust if necessary\n",
    "    updated_model_state = model_state  # Update as needed if you modify the model state\n",
    "\n",
    "    return updated_optimizer_state, updated_variables, updated_model_state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taylor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
